#!/usr/bin/env python3
"""
ğŸ”§ SISTEMA DE TESTING INTELIGENTE Y ADAPTATIVO
=================================================

Sistema que se adapta automÃ¡ticamente a las funciones disponibles
y genera tests robustos para el cÃ³digo real existente.

âœ… Auto-detecciÃ³n de funciones disponibles
âœ… Tests adaptativos basados en el cÃ³digo real
âœ… Cobertura completa y robusta
âœ… Reporte profesional con mÃ©tricas avanzadas
"""

import sys
import os
import time
import importlib
import inspect
from pathlib import Path
from typing import Dict, List, Any
import warnings

# Configurar path
sys.path.append('.')

# Suprimir warnings para output mÃ¡s limpio
warnings.filterwarnings('ignore')

class SmartTester:
    """Tester inteligente que se adapta al cÃ³digo real"""
    
    def __init__(self):
        self.results = {}
        self.test_data = None
        self.available_functions = {}
        
    def create_smart_test_data(self):
        """Crear datos de prueba inteligentes"""
        import pandas as pd
        import numpy as np
        
        np.random.seed(42)
        
        # Datos realistas para criptomonedas
        n_obs = 200
        data = {
            'id': ['BTC', 'ETH', 'ADA', 'SOL', 'MATIC'] * (n_obs // 5),
            'symbol': [f'SYM{i%10}' for i in range(n_obs)],
            'name': [f'Token {i%20}' for i in range(n_obs)],
            'narrative': ['defi', 'gaming', 'ai', 'meme', 'rwa', 'infrastructure'] * (n_obs // 6 + 1),
            'close': np.random.lognormal(8, 1, n_obs),
            'market_cap': np.random.lognormal(25, 2, n_obs),
            'volume': np.random.lognormal(20, 1.5, n_obs),
            'date': pd.date_range('2023-01-01', periods=n_obs, freq='D')
        }
        
        # Ajustar longitudes para que coincidan
        for key in data:
            if len(data[key]) > n_obs:
                data[key] = data[key][:n_obs]
            elif len(data[key]) < n_obs:
                # Repetir hasta completar
                while len(data[key]) < n_obs:
                    data[key].extend(data[key][:min(len(data[key]), n_obs - len(data[key]))])
        
        df = pd.DataFrame(data)
        
        # Agregar algunos NaN para testing
        nan_indices = np.random.choice(df.index, 10, replace=False)
        df.loc[nan_indices[:5], 'market_cap'] = np.nan
        
        self.test_data = df
        print(f"ğŸ“Š Datos de prueba creados: {len(df)} observaciones, {df['id'].nunique()} tokens Ãºnicos")
        return df
    
    def discover_available_functions(self, module_name: str):
        """Descubrir funciones disponibles en un mÃ³dulo"""
        try:
            module = importlib.import_module(f'utils.{module_name}')
            functions = {}
            
            for name, obj in inspect.getmembers(module):
                if inspect.isfunction(obj) and not name.startswith('_'):
                    sig = inspect.signature(obj)
                    functions[name] = {
                        'callable': obj,
                        'signature': sig,
                        'parameters': list(sig.parameters.keys()),
                        'docstring': obj.__doc__ or 'Sin documentaciÃ³n'
                    }
            
            self.available_functions[module_name] = functions
            print(f"ğŸ” MÃ³dulo {module_name}: {len(functions)} funciones descubiertas")
            
            return functions
            
        except ImportError as e:
            print(f"âŒ Error importando {module_name}: {e}")
            return {}
    
    def test_data_analysis_smart(self):
        """Tests inteligentes para data_analysis"""
        functions = self.discover_available_functions('data_analysis')
        
        tests_passed = 0
        total_tests = 0
        
        print("\nğŸ”¬ === Testing data_analysis (Inteligente) ===")
        
        # Test de importaciÃ³n
        total_tests += 1
        if functions:
            print("   âœ… Test 1: MÃ³dulo importado exitosamente")
            tests_passed += 1
        else:
            print("   âŒ Test 1: Error en importaciÃ³n del mÃ³dulo")
        
        # Test calculate_basic_metrics
        total_tests += 1
        if 'calculate_basic_metrics' in functions:
            try:
                metrics = functions['calculate_basic_metrics']['callable'](self.test_data)
                if isinstance(metrics, dict) and 'total_observations' in metrics:
                    print(f"   âœ… Test 2: MÃ©tricas bÃ¡sicas - {metrics['total_observations']} obs, {metrics.get('total_tokens', 'N/A')} tokens")
                    tests_passed += 1
                else:
                    print("   âŒ Test 2: Error en estructura de mÃ©tricas")
            except Exception as e:
                print(f"   âŒ Test 2: Error en mÃ©tricas bÃ¡sicas - {e}")
        else:
            print("   âš ï¸  Test 2: FunciÃ³n calculate_basic_metrics no disponible")
        
        # Test outliers (adaptativo)
        total_tests += 1
        outlier_functions = [name for name in functions.keys() if 'outlier' in name.lower()]
        if outlier_functions:
            try:
                func_name = outlier_functions[0]
                outliers = functions[func_name]['callable'](self.test_data, 'close')
                outlier_count = len(outliers) if hasattr(outliers, '__len__') else 0
                print(f"   âœ… Test 3: Outliers detectados con {func_name} - {outlier_count} valores")
                tests_passed += 1
            except Exception as e:
                print(f"   âŒ Test 3: Error en detecciÃ³n de outliers - {e}")
        else:
            print("   âš ï¸  Test 3: No hay funciones de detecciÃ³n de outliers disponibles")
        
        # Test evaluaciÃ³n de calidad
        total_tests += 1
        if 'evaluate_data_quality' in functions:
            try:
                from utils.config import QUALITY_THRESHOLDS
                metrics = functions['calculate_basic_metrics']['callable'](self.test_data)
                quality = functions['evaluate_data_quality']['callable'](metrics, QUALITY_THRESHOLDS)
                if isinstance(quality, dict) and 'overall_status' in quality:
                    print(f"   âœ… Test 4: Calidad evaluada - {quality['overall_status']}")
                    tests_passed += 1
                else:
                    print("   âŒ Test 4: Error en estructura de calidad")
            except Exception as e:
                print(f"   âŒ Test 4: Error en evaluaciÃ³n de calidad - {e}")
        else:
            print("   âš ï¸  Test 4: FunciÃ³n evaluate_data_quality no disponible")
        
        # Test dominancia de mercado
        total_tests += 1
        if 'calculate_market_dominance' in functions:
            try:
                dominance = functions['calculate_market_dominance']['callable'](self.test_data)
                if hasattr(dominance, '__len__') and len(dominance) > 0:
                    print(f"   âœ… Test 5: Dominancia calculada - {len(dominance)} grupos")
                    tests_passed += 1
                else:
                    print("   âŒ Test 5: Error en dominancia")
            except Exception as e:
                print(f"   âŒ Test 5: Error en dominancia - {e}")
        else:
            print("   âš ï¸  Test 5: FunciÃ³n calculate_market_dominance no disponible")
        
        # Test reporte
        total_tests += 1
        if 'generate_summary_report' in functions:
            try:
                metrics = functions['calculate_basic_metrics']['callable'](self.test_data)
                from utils.config import QUALITY_THRESHOLDS
                quality = functions['evaluate_data_quality']['callable'](metrics, QUALITY_THRESHOLDS)
                report = functions['generate_summary_report']['callable'](metrics, quality)
                if isinstance(report, str) and len(report) > 50:
                    print(f"   âœ… Test 6: Reporte generado - {len(report)} caracteres")
                    tests_passed += 1
                else:
                    print("   âŒ Test 6: Error en reporte")
            except Exception as e:
                print(f"   âŒ Test 6: Error en reporte - {e}")
        else:
            print("   âš ï¸  Test 6: FunciÃ³n generate_summary_report no disponible")
        
        success_rate = (tests_passed / total_tests) * 100
        print(f"   ğŸ“Š data_analysis: {tests_passed}/{total_tests} ({success_rate:.1f}%)")
        
        return tests_passed, total_tests
    
    def test_feature_engineering_smart(self):
        """Tests inteligentes para feature_engineering"""
        functions = self.discover_available_functions('feature_engineering')
        
        tests_passed = 0
        total_tests = 0
        
        print("\nğŸ”§ === Testing feature_engineering (Inteligente) ===")
        
        # Test de importaciÃ³n
        total_tests += 1
        if functions:
            print("   âœ… Test 1: MÃ³dulo importado exitosamente")
            tests_passed += 1
        else:
            print("   âŒ Test 1: Error en importaciÃ³n del mÃ³dulo")
        
        # Test calculate_returns
        total_tests += 1
        if 'calculate_returns' in functions:
            try:
                df_returns = functions['calculate_returns']['callable'](self.test_data)
                return_cols = [col for col in df_returns.columns if 'ret_' in col]
                if len(return_cols) > 0:
                    print(f"   âœ… Test 2: Retornos calculados - {return_cols}")
                    tests_passed += 1
                else:
                    print("   âŒ Test 2: No se generaron retornos")
            except Exception as e:
                print(f"   âŒ Test 2: Error en retornos - {e}")
        else:
            print("   âš ï¸  Test 2: FunciÃ³n calculate_returns no disponible")
        
        # Test moving averages
        total_tests += 1
        ma_functions = [name for name in functions.keys() if 'moving' in name.lower() or 'ma' in name.lower()]
        if ma_functions:
            try:
                func_name = ma_functions[0]
                df_ma = functions[func_name]['callable'](self.test_data)
                ma_cols = [col for col in df_ma.columns if any(x in col for x in ['sma_', 'ma_', 'ema_'])]
                if len(ma_cols) > 0:
                    print(f"   âœ… Test 3: Medias mÃ³viles con {func_name} - {ma_cols}")
                    tests_passed += 1
                else:
                    print(f"   âŒ Test 3: No se generaron medias mÃ³viles con {func_name}")
            except Exception as e:
                print(f"   âŒ Test 3: Error en medias mÃ³viles - {e}")
        else:
            print("   âš ï¸  Test 3: No hay funciones de medias mÃ³viles disponibles")
        
        # Test volatilidad
        total_tests += 1
        if 'calculate_volatility' in functions:
            try:
                # Primero calculamos retornos
                df_returns = functions['calculate_returns']['callable'](self.test_data)
                df_vol = functions['calculate_volatility']['callable'](df_returns)
                vol_cols = [col for col in df_vol.columns if 'vol_' in col]
                if len(vol_cols) > 0:
                    print(f"   âœ… Test 4: Volatilidad calculada - {vol_cols}")
                    tests_passed += 1
                else:
                    print("   âŒ Test 4: No se calculÃ³ volatilidad")
            except Exception as e:
                print(f"   âŒ Test 4: Error en volatilidad - {e}")
        else:
            print("   âš ï¸  Test 4: FunciÃ³n calculate_volatility no disponible")
        
        # Test technical features
        total_tests += 1
        tech_functions = [name for name in functions.keys() if 'technical' in name.lower() or 'feature' in name.lower()]
        if tech_functions:
            try:
                func_name = tech_functions[0]
                from utils.config import TECHNICAL_FEATURES
                df_tech = functions[func_name]['callable'](self.test_data, TECHNICAL_FEATURES)
                new_cols = len(df_tech.columns) - len(self.test_data.columns)
                if new_cols > 0:
                    print(f"   âœ… Test 5: Features tÃ©cnicos con {func_name} - {new_cols} nuevas columnas")
                    tests_passed += 1
                else:
                    print(f"   âŒ Test 5: No se agregaron features con {func_name}")
            except Exception as e:
                print(f"   âŒ Test 5: Error en features tÃ©cnicos - {e}")
        else:
            print("   âš ï¸  Test 5: No hay funciones de features tÃ©cnicos disponibles")
        
        success_rate = (tests_passed / total_tests) * 100
        print(f"   ğŸ“Š feature_engineering: {tests_passed}/{total_tests} ({success_rate:.1f}%)")
        
        return tests_passed, total_tests
    
    def test_visualizations_smart(self):
        """Tests inteligentes para visualizations"""
        functions = self.discover_available_functions('visualizations')
        
        tests_passed = 0
        total_tests = 0
        
        print("\nğŸ“Š === Testing visualizations (Inteligente) ===")
        
        # Test de importaciÃ³n
        total_tests += 1
        if functions:
            print("   âœ… Test 1: MÃ³dulo importado exitosamente")
            tests_passed += 1
        else:
            print("   âŒ Test 1: Error en importaciÃ³n del mÃ³dulo")
        
        # Test para cada funciÃ³n de visualizaciÃ³n
        plot_functions = [name for name in functions.keys() if name.startswith('plot_')]
        
        for func_name in plot_functions:
            total_tests += 1
            try:
                from utils.config import NARRATIVE_COLORS
                
                # Intentar con diferentes combinaciones de parÃ¡metros
                func_info = functions[func_name]
                params = func_info['parameters']
                
                if 'colors' in params:
                    fig = func_info['callable'](self.test_data, NARRATIVE_COLORS)
                else:
                    fig = func_info['callable'](self.test_data)
                
                if fig is not None:
                    print(f"   âœ… Test {total_tests}: {func_name} generado exitosamente")
                    tests_passed += 1
                else:
                    print(f"   âŒ Test {total_tests}: {func_name} retornÃ³ None")
                    
            except Exception as e:
                print(f"   âŒ Test {total_tests}: Error en {func_name} - {str(e)[:50]}...")
        
        if not plot_functions:
            total_tests += 1
            print("   âš ï¸  Test 2: No se encontraron funciones de visualizaciÃ³n")
        
        success_rate = (tests_passed / total_tests) * 100
        print(f"   ğŸ“Š visualizations: {tests_passed}/{total_tests} ({success_rate:.1f}%)")
        
        return tests_passed, total_tests
    
    def test_config_smart(self):
        """Tests inteligentes para config"""
        print("\nâš™ï¸ === Testing config (Inteligente) ===")
        
        tests_passed = 0
        total_tests = 0
        
        # Test de importaciÃ³n bÃ¡sica
        total_tests += 1
        try:
            import utils.config
            print("   âœ… Test 1: MÃ³dulo config importado exitosamente")
            tests_passed += 1
        except ImportError as e:
            print(f"   âŒ Test 1: Error importando config - {e}")
        
        # Descubrir quÃ© variables estÃ¡n disponibles
        config_vars = {}
        try:
            import utils.config as config
            config_vars = {name: getattr(config, name) for name in dir(config) if not name.startswith('_')}
        except:
            pass
        
        # Test variables importantes
        important_vars = ['NARRATIVE_COLORS', 'QUALITY_THRESHOLDS', 'ANALYSIS_CONFIG', 'TECHNICAL_FEATURES']
        
        for var_name in important_vars:
            total_tests += 1
            if var_name in config_vars:
                var_value = config_vars[var_name]
                if isinstance(var_value, dict) and len(var_value) > 0:
                    print(f"   âœ… Test {total_tests}: {var_name} configurado - {len(var_value)} elementos")
                    tests_passed += 1
                elif isinstance(var_value, (list, tuple)) and len(var_value) > 0:
                    print(f"   âœ… Test {total_tests}: {var_name} configurado - {len(var_value)} elementos")
                    tests_passed += 1
                else:
                    print(f"   âš ï¸  Test {total_tests}: {var_name} existe pero estÃ¡ vacÃ­o")
            else:
                print(f"   âš ï¸  Test {total_tests}: {var_name} no encontrado")
        
        success_rate = (tests_passed / total_tests) * 100
        print(f"   ğŸ“Š config: {tests_passed}/{total_tests} ({success_rate:.1f}%)")
        
        return tests_passed, total_tests
    
    def run_comprehensive_tests(self):
        """Ejecutar todos los tests inteligentes"""
        start_time = time.time()
        
        print("ğŸ§ " + "="*60)
        print("ğŸš€ SISTEMA DE TESTING INTELIGENTE Y ADAPTATIVO")
        print("ğŸ§ " + "="*60)
        
        # Crear datos de prueba
        self.create_smart_test_data()
        
        # Ejecutar tests inteligentes
        all_results = []
        
        all_results.append(self.test_data_analysis_smart())
        all_results.append(self.test_feature_engineering_smart())
        all_results.append(self.test_visualizations_smart())
        all_results.append(self.test_config_smart())
        
        # Calcular estadÃ­sticas finales
        total_passed = sum(result[0] for result in all_results)
        total_tests = sum(result[1] for result in all_results)
        overall_success = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        execution_time = time.time() - start_time
        
        # Reporte final
        print("\nğŸ" + "="*60)
        print("ğŸ“Š REPORTE FINAL INTELIGENTE")
        print("ğŸ" + "="*60)
        print(f"â±ï¸  Tiempo de ejecuciÃ³n: {execution_time:.2f}s")
        print(f"ğŸ“ˆ Tasa de Ã©xito adaptativa: {overall_success:.1f}%")
        print(f"âœ… Tests pasados: {total_passed}")
        print(f"âŒ Tests fallidos: {total_tests - total_passed}")
        print(f"ğŸ§ª Total tests ejecutados: {total_tests}")
        
        print(f"\nğŸ” Funciones descubiertas por mÃ³dulo:")
        for module_name, functions in self.available_functions.items():
            print(f"   ğŸ“¦ {module_name}: {len(functions)} funciones")
            for func_name in list(functions.keys())[:3]:  # Mostrar las primeras 3
                print(f"      â€¢ {func_name}")
            if len(functions) > 3:
                print(f"      â€¢ ... y {len(functions) - 3} mÃ¡s")
        
        print("\nğŸ¯" + "="*60)
        if overall_success >= 90:
            print("ğŸ‰ Â¡EXCELENTE! Sistema completamente funcional y bien estructurado")
        elif overall_success >= 75:
            print("ğŸ‘ Â¡BUENO! Sistema funcional con estructura sÃ³lida")
        elif overall_success >= 60:
            print("âœ… FUNCIONAL. Sistema operativo con algunas limitaciones")
        else:
            print("âš ï¸  BÃSICO. Sistema funciona pero necesita expansiÃ³n")
        print("ğŸ¯" + "="*60)
        
        return overall_success, total_passed, total_tests

def main():
    """FunciÃ³n principal"""
    tester = SmartTester()
    tester.run_comprehensive_tests()

if __name__ == "__main__":
    main()
