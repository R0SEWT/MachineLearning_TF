#!/bin/bash

# ====================================================================
# ðŸŒ™ EXPERIMENTO NOCTURNO GPU - OPTIMIZACIÃ“N COMPLETA
# Sistema automÃ¡tico de optimizaciÃ³n de hiperparÃ¡metros usando GPU
# ====================================================================

# ConfiguraciÃ³n
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LOG_DIR="$SCRIPT_DIR/../../optimization_results/logs"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
MAIN_LOG="$LOG_DIR/experimento_nocturno_gpu_$TIMESTAMP.log"

# Crear directorio de logs
mkdir -p "$LOG_DIR"

# FunciÃ³n de logging
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$MAIN_LOG"
}

# FunciÃ³n para manejar errores
handle_error() {
    log "âŒ ERROR: $1"
    log "ðŸ” Revisar logs para mÃ¡s detalles"
    exit 1
}

# FunciÃ³n para mostrar progreso
show_progress() {
    echo "=================================================="
    echo "ðŸš€ $1"
    echo "â° $(date '+%Y-%m-%d %H:%M:%S')"
    echo "=================================================="
}

# Verificar GPU antes de empezar
check_gpu() {
    log "ðŸ”¥ Verificando GPU disponible..."
    nvidia-smi --query-gpu=name,memory.total,utilization.gpu --format=csv,noheader,nounits >> "$MAIN_LOG"
    if [ $? -ne 0 ]; then
        handle_error "GPU no disponible o nvidia-smi fallÃ³"
    fi
    log "âœ… GPU verificada y disponible"
}

# Inicio del experimento
show_progress "EXPERIMENTO NOCTURNO GPU INICIADO"
log "ðŸŒ™ðŸš€ Experimento nocturno GPU iniciado"
log "ðŸ“ Directorio: $SCRIPT_DIR"
log "ðŸ“ Log principal: $MAIN_LOG"

# Verificar GPU
check_gpu

# Activar ambiente conda
log "ðŸ”§ Activando ambiente conda ML-TF-G"
source ~/miniconda3/etc/profile.d/conda.sh
conda activate ML-TF-G || handle_error "No se pudo activar el ambiente conda"

# Test GPU rÃ¡pido
show_progress "VERIFICACIÃ“N GPU Y SISTEMA"
log "ðŸ§ª Ejecutando tests de GPU y sistema..."
python test_gpu.py > "$LOG_DIR/gpu_test_$TIMESTAMP.log" 2>&1
if [ $? -ne 0 ]; then
    handle_error "Tests de GPU fallaron"
fi

python test_ml_system.py > "$LOG_DIR/system_test_$TIMESTAMP.log" 2>&1
if [ $? -ne 0 ]; then
    handle_error "Tests del sistema fallaron"
fi
log "âœ… GPU y sistema verificados correctamente"

# ====================================================================
# FASE 1: OPTIMIZACIÃ“N INTENSIVA GPU (1-2 horas)
# ====================================================================
show_progress "FASE 1: OPTIMIZACIÃ“N INTENSIVA GPU (1-2 horas)"

log "ðŸ”¥ OptimizaciÃ³n XGBoost GPU (intensiva)..."
python quick_optimization.py --mode experimental --trials 300 --timeout 2400 > "$LOG_DIR/intensive_xgb_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… XGBoost GPU intensivo completado"
else
    log "âš ï¸ XGBoost GPU intensivo tuvo problemas, continuando..."
fi

log "ðŸ’¡ OptimizaciÃ³n LightGBM GPU (intensiva)..."
python quick_optimization.py --mode experimental --trials 300 --timeout 2400 > "$LOG_DIR/intensive_lgb_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… LightGBM GPU intensivo completado"
else
    log "âš ï¸ LightGBM GPU intensivo tuvo problemas, continuando..."
fi

log "ðŸ± OptimizaciÃ³n CatBoost GPU (intensiva)..."
python quick_optimization.py --mode experimental --trials 300 --timeout 2400 > "$LOG_DIR/intensive_cat_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… CatBoost GPU intensivo completado"
else
    log "âš ï¸ CatBoost GPU intensivo tuvo problemas, continuando..."
fi

# ====================================================================
# FASE 2: OPTIMIZACIÃ“N ULTRA-PROFUNDA (4-6 horas)
# ====================================================================
show_progress "FASE 2: OPTIMIZACIÃ“N ULTRA-PROFUNDA GPU (4-6 horas)"

log "ðŸŒŸ Ejecutando optimizaciÃ³n ultra-profunda GPU (500 trials, 3 horas por modelo)..."
python quick_optimization.py --mode experimental --trials 500 --timeout 10800 > "$LOG_DIR/ultra_deep_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… OptimizaciÃ³n ultra-profunda GPU completada"
else
    log "âš ï¸ OptimizaciÃ³n ultra-profunda GPU tuvo problemas, continuando..."
fi

# ====================================================================
# FASE 3: OPTIMIZACIÃ“N EXTREMA NOCTURNA (6-8 horas)
# ====================================================================
show_progress "FASE 3: OPTIMIZACIÃ“N EXTREMA NOCTURNA GPU (6-8 horas)"

log "ðŸš€ Ejecutando optimizaciÃ³n extrema nocturna (1000 trials, 4 horas por modelo)..."
python quick_optimization.py --mode experimental --trials 1000 --timeout 14400 > "$LOG_DIR/extreme_nocturna_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… OptimizaciÃ³n extrema nocturna completada"
else
    log "âš ï¸ OptimizaciÃ³n extrema nocturna tuvo problemas, continuando..."
fi

# ====================================================================
# FASE 4: ANÃLISIS Y PROCESAMIENTO DE RESULTADOS
# ====================================================================
show_progress "FASE 4: ANÃLISIS DE RESULTADOS GPU"

log "ðŸ“Š Ejecutando anÃ¡lisis completo de resultados..."
python optuna_results_analyzer.py > "$LOG_DIR/results_analysis_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… AnÃ¡lisis de resultados completado"
else
    log "âš ï¸ AnÃ¡lisis tuvo problemas, continuando..."
fi

log "ðŸ”— Integrando mejores parÃ¡metros optimizados por GPU..."
python integrate_optimized_params.py > "$LOG_DIR/integration_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… IntegraciÃ³n completada"
else
    log "âš ï¸ IntegraciÃ³n tuvo problemas, continuando..."
fi

# ====================================================================
# FASE 5: VALIDACIÃ“N FINAL GPU
# ====================================================================
show_progress "FASE 5: VALIDACIÃ“N FINAL GPU"

log "ðŸŽ¯ Ejecutando entrenamiento final con mejores parÃ¡metros GPU..."
timeout 3600 python crypto_ml_trainer_optimized.py > "$LOG_DIR/final_training_gpu_$TIMESTAMP.log" 2>&1
if [ $? -eq 0 ]; then
    log "âœ… Entrenamiento final GPU completado exitosamente"
else
    log "âš ï¸ Entrenamiento final GPU tuvo timeout o problemas"
fi

# ====================================================================
# MONITOREO GPU FINAL
# ====================================================================
show_progress "MONITOREO GPU FINAL"

log "ðŸ“Š Estado final de GPU:"
nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu,temperature.gpu --format=csv,noheader,nounits >> "$MAIN_LOG"

# ====================================================================
# RESUMEN FINAL GPU
# ====================================================================
show_progress "EXPERIMENTO NOCTURNO GPU COMPLETADO"

END_TIME=$(date +"%Y%m%d_%H%M%S")
log "ðŸŽ‰ðŸš€ Experimento nocturno GPU finalizado"
log "â° Tiempo de finalizaciÃ³n: $(date '+%Y-%m-%d %H:%M:%S')"

# Generar resumen especializado para GPU
SUMMARY_FILE="$LOG_DIR/resumen_experimento_gpu_$TIMESTAMP.txt"
cat > "$SUMMARY_FILE" << EOF
ðŸŒ™ðŸš€ RESUMEN DEL EXPERIMENTO NOCTURNO GPU
=========================================

â° Inicio: $TIMESTAMP
â° Fin: $END_TIME

ðŸš€ GPU UTILIZADA:
$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits)

ðŸ“ Logs generados:
$(ls -la "$LOG_DIR"/*gpu*$TIMESTAMP.* 2>/dev/null | tail -10)

ðŸ“Š Archivos de resultados:
$(ls -la ../../optimization_results/*.json 2>/dev/null | tail -5)

ðŸŽ¯ VENTAJAS DE GPU OBTENIDAS:
- OptimizaciÃ³n mÃ¡s rÃ¡pida con datasets grandes
- Mayor paralelizaciÃ³n en cross-validation
- Mejor exploraciÃ³n del espacio de hiperparÃ¡metros
- Entrenamiento acelerado de modelos complejos

ðŸ“ˆ Para usar resultados:
1. python crypto_ml_trainer_optimized.py
2. Revisar integration_report.md
3. Los modelos ya estÃ¡n configurados para GPU

ðŸ“ Logs principales GPU:
- GPU Test: $LOG_DIR/gpu_test_$TIMESTAMP.log
- Intensivo GPU: $LOG_DIR/intensive_*_gpu_$TIMESTAMP.log
- Ultra-profundo: $LOG_DIR/ultra_deep_gpu_$TIMESTAMP.log
- Extremo nocturno: $LOG_DIR/extreme_nocturna_gpu_$TIMESTAMP.log
- Entrenamiento final: $LOG_DIR/final_training_gpu_$TIMESTAMP.log

ðŸŽ‰ Experimento GPU completado!
EOF

log "ðŸ“„ Resumen GPU guardado en: $SUMMARY_FILE"

# Mostrar estadÃ­sticas finales
echo ""
echo "ðŸŒŸðŸš€ ============================================="
echo "ðŸŒŸðŸš€ EXPERIMENTO NOCTURNO GPU COMPLETADO"
echo "ðŸŒŸðŸš€ ============================================="
echo ""
echo "ðŸ“Š GPU Final Status:"
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits
echo ""
echo "ðŸ“‚ Archivos generados:"
ls -la "$LOG_DIR"/*gpu*$TIMESTAMP.* 2>/dev/null || echo "No se encontraron logs GPU"
echo ""
echo "ðŸŽ¯ PrÃ³ximos pasos:"
echo "1. Revisar: cat $SUMMARY_FILE"
echo "2. Analizar: python optuna_results_analyzer.py"
echo "3. Usar optimizados GPU: python crypto_ml_trainer_optimized.py"
echo ""
echo "âœ…ðŸš€ Â¡Todo listo para revisar los resultados GPU!"

log "ðŸŒŸðŸš€ Script de experimento nocturno GPU finalizado exitosamente"
