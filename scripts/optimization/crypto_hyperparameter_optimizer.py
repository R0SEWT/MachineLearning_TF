#!/usr/bin/env python3
"""
Sistema de Optimizaci√≥n de Hiperpar√°metros con Optuna para Criptomonedas
Optimizaci√≥n autom√°tica de XGBoost, LightGBM y CatBoost
"""

import sys
import os
import pandas as pd
import numpy as np
import warnings
from pathlib import Path
from datetime import datetime
import json
import pickle
from typing import Dict, Any, List, Tuple, Optional
import time

# Suprimir warnings
warnings.filterwarnings('ignore')

# Importar nuevos componentes de Fase 1
try:
    from config.optimization_config import CONFIG, MODEL_CONFIG
    from utils.gpu_manager import GPU_MANAGER
    from utils.data_validator import DataValidator, DataValidationError
    from utils.metrics_calculator import MetricsCalculator, MetricsResult
    from utils.optimization_logger import get_optimization_logger
    print("‚úÖ Nuevos componentes de Fase 1 importados correctamente")
except ImportError as e:
    print(f"‚ö†Ô∏è Error importando componentes de Fase 1: {e}")
    print("Continuando con funcionalidad b√°sica...")

# Importar nuevos componentes de Fase 2
try:
    from config.optuna_config import (
        SAMPLER_CONFIG, PRUNER_CONFIG, MULTI_OBJECTIVE_CONFIG, CONVERGENCE_CONFIG,
        SAMPLER_FACTORY, PRUNER_FACTORY, STRATEGY_SELECTOR
    )
    from utils.temporal_validator import TEMPORAL_VALIDATOR, TimeSeriesValidationConfig
    from utils.early_stopping import ADAPTIVE_CONTROLLER, EarlyStoppingConfig
    print("‚úÖ Nuevos componentes de Fase 2 importados correctamente")
except ImportError as e:
    print(f"‚ö†Ô∏è Error importando componentes de Fase 2: {e}")
    print("Continuando con funcionalidad de Fase 1...")

# Importar nuevos componentes de Fase 3
try:
    from utils.parallelization import (
        WORKER_MANAGER, DISTRIBUTED_OPTIMIZER, PARALLEL_TRIAL_EXECUTOR,
        ParallelizationConfig, DEFAULT_PARALLELIZATION_CONFIG
    )
    from utils.memory_manager import (
        MEMORY_MANAGER, MemoryConfig, DEFAULT_MEMORY_CONFIG
    )
    print("‚úÖ Nuevos componentes de Fase 3 importados correctamente")
except ImportError as e:
    print(f"‚ö†Ô∏è Error importando componentes de Fase 3: {e}")
    print("Continuando con funcionalidad de Fase 2...")

# Agregar paths necesarios
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src', 'utils', 'utils'))

# Intentar importar feature engineering
try:
    from src.utils.utils.feature_engineering import create_ml_features, prepare_ml_dataset
    print("‚úÖ Feature engineering importado desde src.utils.utils")
except ImportError:
    try:
        from utils.utils.feature_engineering import create_ml_features, prepare_ml_dataset
        print("‚úÖ Feature engineering importado desde utils.utils")
    except ImportError:
        try:
            from feature_engineering import create_ml_features, prepare_ml_dataset
            print("‚úÖ Feature engineering importado desde feature_engineering")
        except ImportError:
            print("‚ùå No se pudo importar feature_engineering")
            sys.exit(1)

# Imports de ML
try:
    from sklearn.model_selection import cross_val_score, StratifiedKFold
    from sklearn.metrics import roc_auc_score, classification_report
    import xgboost as xgb
    import lightgbm as lgb
    import catboost as cb
    import optuna
    from optuna.visualization import plot_optimization_history, plot_param_importances, plot_contour
    print("‚úÖ Todas las librer√≠as importadas correctamente")
except ImportError as e:
    print(f"‚ùå Error importando librer√≠as: {e}")
    sys.exit(1)

class CryptoHyperparameterOptimizer:
    """
    Sistema completo de optimizaci√≥n de hiperpar√°metros para modelos de criptomonedas
    Incluye mejoras de Fase 1: validaci√≥n robusta, GPU inteligente, m√©tricas m√∫ltiples y logging
    """
    
    def __init__(self, data_path: str = None, results_path: str = None, config: Any = None,
                 parallelization_config: ParallelizationConfig = None, 
                 memory_config: MemoryConfig = None):
        """
        Inicializar el optimizador con mejoras de Fase 1, Fase 2 y Fase 3
        
        Args:
            data_path: Ruta a los datos (opcional, usa CONFIG si no se especifica)
            results_path: Ruta donde guardar resultados (opcional)
            config: Configuraci√≥n personalizada (opcional)
            parallelization_config: Configuraci√≥n de paralelizaci√≥n (opcional)
            memory_config: Configuraci√≥n de memoria (opcional)
        """
        # Usar configuraci√≥n global o personalizada
        self.config = config if config is not None else CONFIG
        
        # Rutas de datos y resultados
        self.data_path = data_path or self.config.data_path
        self.results_path = Path(results_path or self.config.results_path)
        self.results_path.mkdir(exist_ok=True)
        
        # Inicializar componentes de Fase 1
        try:
            # Logger estructurado
            self.logger = get_optimization_logger(
                log_dir=str(self.results_path / "logs"),
                log_level=self.config.log_level,
                enable_file_logging=self.config.log_to_file
            )
            
            # GPU Manager
            self.gpu_manager = GPU_MANAGER
            
            # Validador de datos
            self.data_validator = DataValidator(self.config)
            
            # Calculadora de m√©tricas
            self.metrics_calculator = MetricsCalculator(
                primary_metric=self.config.primary_metric
            )
            
            print("‚úÖ Componentes de Fase 1 inicializados correctamente")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error inicializando componentes de Fase 1: {e}")
            print("Continuando con funcionalidad b√°sica...")
            self.logger = None
            self.gpu_manager = None
            self.data_validator = None
            self.metrics_calculator = None
        
        # Inicializar componentes de Fase 2
        try:
            # Validador temporal
            self.temporal_validator = TEMPORAL_VALIDATOR
            
            # Controlador adaptativo
            self.adaptive_controller = ADAPTIVE_CONTROLLER
            
            # Configuraciones de Optuna
            self.sampler_factory = SAMPLER_FACTORY
            self.pruner_factory = PRUNER_FACTORY
            self.strategy_selector = STRATEGY_SELECTOR
            
            # Configuraci√≥n de validaci√≥n temporal
            self.temporal_config = TimeSeriesValidationConfig()
            
            # Configuraci√≥n de early stopping
            self.early_stopping_config = EarlyStoppingConfig()
            
            print("‚úÖ Componentes de Fase 2 inicializados correctamente")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error inicializando componentes de Fase 2: {e}")
            print("Continuando con funcionalidad de Fase 1...")
            self.temporal_validator = None
            self.adaptive_controller = None
            self.sampler_factory = None
            self.pruner_factory = None
            self.strategy_selector = None
        
        # Inicializar componentes de Fase 3
        try:
            # Configuraciones de Fase 3
            self.parallelization_config = parallelization_config or DEFAULT_PARALLELIZATION_CONFIG
            self.memory_config = memory_config or DEFAULT_MEMORY_CONFIG
            
            # Gestor de workers y paralelizaci√≥n
            self.worker_manager = WORKER_MANAGER
            self.distributed_optimizer = DISTRIBUTED_OPTIMIZER
            self.parallel_trial_executor = PARALLEL_TRIAL_EXECUTOR
            
            # Gestor de memoria
            self.memory_manager = MEMORY_MANAGER
            
            print("‚úÖ Componentes de Fase 3 inicializados correctamente")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error inicializando componentes de Fase 3: {e}")
            print("Continuando con funcionalidad de Fase 2...")
            self.worker_manager = None
            self.distributed_optimizer = None
            self.parallel_trial_executor = None
            self.memory_manager = None
        
        # Datasets
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None
        
        # Estudios de Optuna
        self.studies = {}
        self.best_params = {}
        self.best_scores = {}
        self.detailed_results = {}  # Para m√©tricas m√∫ltiples
        self.convergence_history = {}  # Para an√°lisis de convergencia
        
        # Configuraci√≥n desde CONFIG
        self.cv_folds = self.config.cv_folds
        self.random_state = self.config.random_state
        
        # Log inicio
        if self.logger:
            self.logger.log_optimization_start({
                'data_path': self.data_path,
                'results_path': str(self.results_path),
                'cv_folds': self.cv_folds,
                'random_state': self.random_state,
                'phase_1_enabled': self.data_validator is not None,
                'phase_2_enabled': self.temporal_validator is not None,
                'phase_3_enabled': self.worker_manager is not None and self.memory_manager is not None,
                'parallelization_config': self.parallelization_config.__dict__ if hasattr(self, 'parallelization_config') else None,
                'memory_config': self.memory_config.__dict__ if hasattr(self, 'memory_config') else None
            })
            
            # Log informaci√≥n de hardware
            if self.gpu_manager:
                self.gpu_manager.print_hardware_summary()
                self.logger.log_gpu_info(self.gpu_manager.gpu_info)
        
        print("üîß CryptoHyperparameterOptimizer inicializado")
        print(f"   üìÅ Datos: {self.data_path}")
        print(f"   üìä Resultados: {self.results_path}")
        print(f"   üéØ Fase 1: {'‚úÖ' if self.data_validator is not None else '‚ùå'}")
        print(f"   üöÄ Fase 2: {'‚úÖ' if self.temporal_validator is not None else '‚ùå'}")
        print(f"   ‚ö° Fase 3: {'‚úÖ' if self.worker_manager is not None and self.memory_manager is not None else '‚ùå'}")
        
        # Inicializar sistemas de Fase 3
        if self.memory_manager:
            self.memory_manager.start()
            print(f"   üß† Gesti√≥n de memoria iniciada")
            
        if self.worker_manager:
            print(f"   üë• Workers disponibles: {self.parallelization_config.n_workers}")
            print(f"   üîÑ Tipo de workers: {self.parallelization_config.worker_type}")
            print(f"   üåê Modo distribuido: {self.parallelization_config.distributed_mode}")
        print(f"   üíæ Resultados: {self.results_path}")
        print(f"   üéØ M√©trica primaria: {self.config.primary_metric}")
        print(f"   üîÑ CV folds: {self.cv_folds}")
        print(f"   üöÄ Fase 1 (Fundamentos): {'‚úÖ' if self.data_validator else '‚ùå'}")
        print(f"   ‚ö° Fase 2 (Optimizaci√≥n Core): {'‚úÖ' if self.temporal_validator else '‚ùå'}")
    
    def load_and_prepare_data(self, target_period: int = None, min_market_cap: float = None, 
                             max_market_cap: float = None):
        """
        Cargar y preparar datos con validaci√≥n robusta (Fase 1)
        """
        # Usar valores de configuraci√≥n si no se especifican
        target_period = target_period or self.config.target_period
        min_market_cap = min_market_cap or self.config.min_market_cap
        max_market_cap = max_market_cap or self.config.max_market_cap
        
        print("üöÄ======================================================================")
        print("üìä CARGANDO Y PREPARANDO DATOS CON VALIDACI√ìN ROBUSTA")
        print("üöÄ======================================================================")
        
        # FASE 1: VALIDACI√ìN ROBUSTA
        try:
            if self.data_validator:
                target_col = f'high_return_{target_period}d'
                
                # Validaci√≥n completa de datos
                validation_results = self.data_validator.run_full_validation(
                    data_path=self.data_path,
                    target_column=target_col,
                    exclude_columns=self.config.exclude_columns
                )
                
                if self.logger:
                    self.logger.log_data_info(validation_results)
                
                print("‚úÖ Validaci√≥n de datos completada exitosamente")
                
            else:
                print("‚ö†Ô∏è Validador no disponible, usando validaci√≥n b√°sica")
                
        except DataValidationError as e:
            error_msg = f"Error de validaci√≥n de datos: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            raise RuntimeError(error_msg)
        except Exception as e:
            error_msg = f"Error inesperado en validaci√≥n: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            print(f"‚ö†Ô∏è {error_msg}")
        
        # Cargar datos
        print(f"üìÅ Cargando datos desde: {self.data_path}")
        if not os.path.exists(self.data_path):
            error_msg = f"No se encontr√≥ el archivo: {self.data_path}"
            if self.logger:
                self.logger.log_error(error_msg)
            raise FileNotFoundError(error_msg)
        
        try:
            df = pd.read_csv(self.data_path)
            print(f"   üìä Datos cargados: {df.shape}")
            
        except Exception as e:
            error_msg = f"Error cargando datos: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            raise RuntimeError(error_msg)
        
        # Filtrar por market cap con validaci√≥n (opcional)
        try:
            if 'market_cap' in df.columns:
                df_filtered = df[(df['market_cap'] >= min_market_cap) & 
                                (df['market_cap'] <= max_market_cap)].copy()
                print(f"   üí∞ Filtrado por market cap: {df_filtered.shape}")
                
                if len(df_filtered) == 0:
                    raise ValueError("No quedan datos despu√©s del filtro de market cap")
            else:
                # No filtrar si no hay columna market_cap (datos sint√©ticos)
                df_filtered = df.copy()
                print(f"   üí∞ Sin filtro de market cap (columna no encontrada): {df_filtered.shape}")
                
        except Exception as e:
            error_msg = f"Error filtrando por market cap: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            raise RuntimeError(error_msg)
        
        # Crear features con manejo de errores
        try:
            # Verificar si es un dataset sint√©tico (ya tiene features)
            if all(col.startswith('feature_') for col in df_filtered.columns if col not in ['date', 'high_return_30d']):
                print("üîß Detectado dataset sint√©tico - usando features existentes...")
                df_features = df_filtered.copy()
            else:
                print("üîß Creando features avanzadas...")
                df_features = create_ml_features(df_filtered, include_targets=True)
            
        except Exception as e:
            error_msg = f"Error creando features: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            
            # Fallback para datos sint√©ticos
            print("   ‚ö†Ô∏è  Fallback: usando datos como features directas")
            df_features = df_filtered.copy()
        
        # Preparar dataset con validaci√≥n
        target_col = f'high_return_{target_period}d'
        print(f"üéØ Variable objetivo: {target_col}")
        
        if target_col not in df_features.columns:
            error_msg = f"Columna objetivo '{target_col}' no encontrada"
            if self.logger:
                self.logger.log_error(error_msg)
            raise ValueError(error_msg)
        
        # Split temporal con configuraci√≥n
        try:
            df_clean = df_features.dropna(subset=[target_col]).sort_values('date')
            
            # Usar configuraci√≥n para splits
            split_config = self.config.get_data_split_config()
            n_total = len(df_clean)
            
            train_end = int(split_config['train_size'] * n_total)
            val_end = int((split_config['train_size'] + split_config['val_size']) * n_total)
            
            df_train = df_clean.iloc[:train_end]
            df_val = df_clean.iloc[train_end:val_end]
            df_test = df_clean.iloc[val_end:]
            
            print(f"   üìä Train: {len(df_train)} ({len(df_train)/n_total:.1%})")
            print(f"   üìä Validation: {len(df_val)} ({len(df_val)/n_total:.1%})")
            print(f"   üìä Test: {len(df_test)} ({len(df_test)/n_total:.1%})")
            
        except Exception as e:
            error_msg = f"Error en split de datos: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            raise RuntimeError(error_msg)
        
        # Preparar features con manejo de errores robusto
        try:
            exclude_cols = self.config.exclude_columns + \
                           [col for col in df_clean.columns if col.startswith('future_') or 
                            col.startswith('high_return_') or col.startswith('return_category_') or
                            col.startswith('extreme_return_')]
            
            feature_cols = [col for col in df_clean.columns if col not in exclude_cols]
            
            def prepare_features(df_subset):
                """Preparar features para un subset con validaci√≥n"""
                X = df_subset[feature_cols].copy()
                
                # Manejar variables categ√≥ricas
                categorical_cols = []
                for col in X.columns:
                    if X[col].dtype == 'object' or col in ['narrative', 'cluster_id']:
                        categorical_cols.append(col)
                
                if categorical_cols:
                    try:
                        from sklearn.preprocessing import LabelEncoder
                        for col in categorical_cols:
                            if col in X.columns:
                                le = LabelEncoder()
                                # Fit con todos los datos para consistencia
                                all_values = pd.concat([df_train[col], df_val[col], df_test[col]]).astype(str)
                                le.fit(all_values)
                                X[col] = le.transform(X[col].astype(str))
                    except ImportError:
                        if self.logger:
                            self.logger.log_warning("sklearn no disponible para LabelEncoder")
                        # Encoding manual b√°sico
                        for col in categorical_cols:
                            if col in X.columns:
                                X[col] = pd.Categorical(X[col]).codes
                
                # Convertir a num√©rico y limpiar con validaci√≥n
                for col in X.columns:
                    X[col] = pd.to_numeric(X[col], errors='coerce')
                
                X = X.fillna(0)
                
                # Limpiar infinitos
                X = X.replace([np.inf, -np.inf], 0)
                
                return X
            
            # Preparar cada subset
            self.X_train = prepare_features(df_train)
            self.X_val = prepare_features(df_val)
            self.X_test = prepare_features(df_test)
            
            self.y_train = df_train[target_col]
            self.y_val = df_val[target_col]
            self.y_test = df_test[target_col]
            
        except Exception as e:
            error_msg = f"Error preparando features: {e}"
            if self.logger:
                self.logger.log_error(error_msg, exception=e)
            raise RuntimeError(error_msg)
        
        # Validaci√≥n final de splits usando DataValidator
        try:
            if self.data_validator:
                split_validation = self.data_validator.validate_data_splits(
                    self.X_train, self.X_val, self.X_test,
                    self.y_train, self.y_val, self.y_test
                )
                
                if self.logger:
                    self.logger.log_info("Validaci√≥n de splits completada", split_validation)
                
        except Exception as e:
            if self.logger:
                self.logger.log_warning(f"Error en validaci√≥n de splits: {e}")
            print(f"‚ö†Ô∏è Error en validaci√≥n de splits: {e}")
        
        # Informaci√≥n final
        print(f"   üîß Features utilizadas: {len(feature_cols)}")
        print(f"   üéØ Distribuci√≥n train: {self.y_train.value_counts().to_dict()}")
        print(f"   üéØ Distribuci√≥n val: {self.y_val.value_counts().to_dict()}")
        print(f"   üéØ Distribuci√≥n test: {self.y_test.value_counts().to_dict()}")
        
        # Log memoria si est√° disponible
        if self.logger and self.data_validator:
            try:
                memory_info = self.data_validator.validate_memory_requirements(self.X_train)
                self.logger.log_memory_usage(memory_info)
            except Exception as e:
                self.logger.log_warning(f"Error verificando memoria: {e}")
        
        if self.logger:
            self.logger.log_info("Preparaci√≥n de datos completada exitosamente", {
                'train_size': len(self.X_train),
                'val_size': len(self.X_val),
                'test_size': len(self.X_test),
                'n_features': len(feature_cols),
                'target_column': target_col
            })
        
        return self
    
    def optimize_xgboost(self, n_trials: int = None, timeout: Optional[int] = None,
                        use_temporal_cv: bool = True, optimization_strategy: str = 'balanced'):
        """
        Optimizar hiperpar√°metros de XGBoost con mejoras de Fase 1 y Fase 2
        
        Args:
            n_trials: N√∫mero de trials (None para usar estrategia)
            timeout: Timeout en segundos (None para usar estrategia)
            use_temporal_cv: Usar validaci√≥n cruzada temporal
            optimization_strategy: Estrategia de optimizaci√≥n ('quick', 'balanced', 'thorough')
        """
        n_trials = n_trials or self.config.default_n_trials
        timeout = timeout or self.config.default_timeout_per_model
        
        print("\nüî•======================================================================")
        print("üî• OPTIMIZANDO XGBOOST CON MEJORAS DE FASE 1 Y FASE 2")
        print("üî•======================================================================")
        
        # Seleccionar estrategia de optimizaci√≥n autom√°ticamente
        if self.strategy_selector:
            strategy_config = self.strategy_selector.select_strategy(
                n_trials=n_trials,
                timeout=timeout,
                problem_type=optimization_strategy
            )
            print(f"   üìã Estrategia seleccionada: {strategy_config}")
        else:
            strategy_config = {'sampler': 'tpe', 'pruner': 'median'}
        
        # Log inicio de optimizaci√≥n del modelo
        if self.logger:
            self.logger.log_model_optimization_start('xgboost', n_trials, {
                'timeout': timeout,
                'cv_folds': self.cv_folds,
                'use_temporal_cv': use_temporal_cv,
                'strategy': strategy_config
            })
        
        model_start_time = time.time()
        
        # Crear sampler y pruner avanzados
        try:
            sampler = self.sampler_factory.create_sampler(
                strategy_config.get('sampler', 'tpe'),
                SAMPLER_CONFIG
            )
            pruner = self.pruner_factory.create_pruner(
                strategy_config.get('pruner', 'median'),
                PRUNER_CONFIG
            )
            print(f"   üéØ Sampler: {type(sampler).__name__}")
            print(f"   ‚úÇÔ∏è  Pruner: {type(pruner).__name__}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error creando sampler/pruner avanzados: {e}")
            sampler = None
            pruner = None
        
        # Obtener monitor de early stopping
        if self.adaptive_controller:
            early_stopping_monitor = self.adaptive_controller.get_monitor('xgboost')
            early_stopping_monitor.reset()
        else:
            early_stopping_monitor = None
        
        def objective(trial):
            """Funci√≥n objetivo para XGBoost con mejoras de Fase 1 y Fase 2"""
            trial_start_time = time.time()
            
            # Verificar early stopping
            if early_stopping_monitor and early_stopping_monitor.stopped:
                raise optuna.TrialPruned()
            
            # Configuraci√≥n base con GPU Manager
            base_params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'random_state': self.random_state,
                'verbosity': 0,
            }
            
            # Configuraci√≥n GPU/CPU inteligente
            if self.gpu_manager:
                try:
                    gpu_config = self.gpu_manager.get_xgboost_config(
                        fallback_to_cpu=self.config.fallback_to_cpu
                    )
                    base_params.update(gpu_config)
                except Exception as e:
                    if self.logger:
                        self.logger.log_warning(f"Error configurando GPU para XGBoost: {e}")
                    base_params.update({'tree_method': 'hist', 'n_jobs': -1})
            else:
                base_params.update({'tree_method': 'hist', 'n_jobs': -1})
            
            # Hiperpar√°metros a optimizar usando MODEL_CONFIG
            xgb_config = MODEL_CONFIG.xgboost_params
            params = base_params.copy()
            
            for param_name, param_config in xgb_config.items():
                if param_config['type'] == 'int':
                    if 'step' in param_config:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], 
                            param_config['high'], step=param_config['step']
                        )
                    else:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], param_config['high']
                        )
                elif param_config['type'] == 'float':
                    params[param_name] = trial.suggest_float(
                        param_name, param_config['low'], 
                        param_config['high'], log=param_config.get('log', False)
                    )
            
            # Log inicio del trial
            if self.logger:
                self.logger.log_trial_start(trial.number, 'xgboost', params)
            
            try:
                # Crear modelo
                model = xgb.XGBClassifier(**params)
                
                # Usar validaci√≥n cruzada temporal si est√° disponible
                if use_temporal_cv and self.temporal_validator and 'date' in self.X_train.columns:
                    # Preparar datos con columna de fecha
                    X_train_with_date = self.X_train.copy()
                    if 'date' not in X_train_with_date.columns:
                        # Si no hay columna de fecha, crear una sint√©tica
                        X_train_with_date['date'] = pd.date_range(start='2020-01-01', periods=len(X_train_with_date), freq='D')
                    
                    try:
                        cv_results = self.temporal_validator.perform_time_series_cv(
                            estimator=model,
                            X=X_train_with_date,
                            y=self.y_train,
                            scoring='roc_auc',
                            cv_type='time_series'
                        )
                        
                        cv_scores = cv_results['scores']
                        primary_score = cv_results['mean_score']
                        
                        # Log m√©tricas de estabilidad temporal
                        if self.logger:
                            self.logger.log_info(f"CV temporal completado - Trial {trial.number}", {
                                'mean_score': primary_score,
                                'std_score': cv_results['std_score'],
                                'stability_score': cv_results['stability_metrics']['stability_score'],
                                'n_folds': cv_results['n_folds']
                            })
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error en CV temporal, usando CV est√°ndar: {e}")
                        # Fallback a CV est√°ndar
                        cv_scores = cross_val_score(
                            model, self.X_train, self.y_train,
                            cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                            scoring='roc_auc',
                            n_jobs=-1
                        )
                        primary_score = cv_scores.mean()
                
                else:
                    # Validaci√≥n cruzada est√°ndar
                    cv_scores = cross_val_score(
                        model, self.X_train, self.y_train,
                        cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                        scoring='roc_auc',
                        n_jobs=-1
                    )
                    primary_score = cv_scores.mean()
                
                # Verificar early stopping inteligente
                if early_stopping_monitor:
                    should_stop = early_stopping_monitor.update(trial.number, primary_score)
                    if should_stop:
                        raise optuna.TrialPruned()
                
                # Calcular m√©tricas m√∫ltiples si est√° disponible
                if self.metrics_calculator:
                    try:
                        # Entrenar en datos completos para evaluaci√≥n
                        model.fit(self.X_train, self.y_train)
                        y_pred = model.predict(self.X_val)
                        y_proba = model.predict_proba(self.X_val)[:, 1]
                        
                        # Calcular todas las m√©tricas
                        metrics_result = self.metrics_calculator.calculate_all_metrics(
                            y_true=self.y_val.values,
                            y_pred=y_pred,
                            y_proba=y_proba,
                            cv_scores=cv_scores.tolist(),
                            metrics_to_calculate=self.config.secondary_metrics
                        )
                        
                        # Log m√©tricas
                        if self.logger:
                            self.logger.log_metrics(trial.number, 'xgboost', metrics_result.secondary_scores)
                        
                        # Report intermediate value para pruning
                        trial.report(primary_score, trial.number)
                        
                        # Verificar si debe ser podado
                        if trial.should_prune():
                            if self.logger:
                                self.logger.log_trial_pruned(trial.number, 'xgboost', "Optuna pruning")
                            raise optuna.TrialPruned()
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error calculando m√©tricas m√∫ltiples: {e}")
                
                # Log trial exitoso
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'xgboost', primary_score, trial_duration)
                
                return primary_score
                
            except optuna.TrialPruned:
                # Re-raise pruned trials
                raise
            except Exception as e:
                # Log trial fallido
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'xgboost', 0.0, trial_duration, "failed")
                    self.logger.log_error(f"Error en trial XGBoost {trial.number}", {'params': params}, e)
                
                raise optuna.TrialPruned()
        
        # Crear y ejecutar estudio con configuraci√≥n avanzada
        study_name = f"xgboost_phase2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        study_kwargs = {
            'direction': 'maximize',
            'study_name': study_name,
            'storage': f'sqlite:///{self.results_path}/optuna_studies.db',
            'load_if_exists': True
        }
        
        # Agregar sampler y pruner si est√°n disponibles
        if sampler:
            study_kwargs['sampler'] = sampler
        if pruner:
            study_kwargs['pruner'] = pruner
        
        study = optuna.create_study(**study_kwargs)
        
        print(f"   üéØ Ejecutando {n_trials} trials con estrategia '{optimization_strategy}'...")
        if timeout:
            print(f"   ‚è∞ Timeout: {timeout} segundos")
        
        # Callback para progreso con early stopping
        def progress_callback(study, trial):
            if trial.number % 10 == 0:
                current_best = study.best_value if study.best_value else 0.0
                
                if self.logger:
                    self.logger.log_progress(trial.number, n_trials, current_best, 'xgboost')
                
                # Verificar early stopping global
                if early_stopping_monitor:
                    should_stop = self.adaptive_controller.should_stop_model(
                        'xgboost', trial.number, current_best
                    )
                    if should_stop:
                        study.stop()
        
        # Ejecutar optimizaci√≥n
        study.optimize(
            objective, 
            n_trials=n_trials, 
            timeout=timeout, 
            callbacks=[progress_callback],
            catch=(Exception,)  # Capturar excepciones para continuar
        )
        
        # Guardar resultados
        model_duration = time.time() - model_start_time
        self.studies['xgboost'] = study
        self.best_params['xgboost'] = study.best_params
        self.best_scores['xgboost'] = study.best_value
        
        # Guardar historial de convergencia
        if early_stopping_monitor:
            self.convergence_history['xgboost'] = early_stopping_monitor.get_summary()
        
        print(f"   ‚úÖ Optimizaci√≥n completada!")
        print(f"   üèÜ Mejor AUC: {study.best_value:.4f}")
        print(f"   üîß Mejores par√°metros: {study.best_params}")
        print(f"   ‚è∞ Tiempo total: {model_duration:.1f}s")
        print(f"   üéØ Trials ejecutados: {len(study.trials)}")
        
        # Informaci√≥n de convergencia
        if early_stopping_monitor:
            convergence_info = early_stopping_monitor.get_summary()
            print(f"   üìä Early stopping: {convergence_info['stopped']}")
            if convergence_info['stopped']:
                print(f"   üõë Raz√≥n: {convergence_info['stop_reason']}")
        
        # Log finalizaci√≥n
        if self.logger:
            self.logger.log_model_optimization_complete(
                'xgboost', study.best_value, study.best_params, model_duration
            )
        
        return study
    
    def optimize_lightgbm(self, n_trials: int = None, timeout: Optional[int] = None,
                         use_temporal_cv: bool = True, optimization_strategy: str = 'balanced'):
        """
        Optimizar hiperpar√°metros de LightGBM con mejoras de Fase 1 y Fase 2
        
        Args:
            n_trials: N√∫mero de trials (None para usar estrategia)
            timeout: Timeout en segundos (None para usar estrategia)
            use_temporal_cv: Usar validaci√≥n cruzada temporal
            optimization_strategy: Estrategia de optimizaci√≥n ('quick', 'balanced', 'thorough')
        """
        n_trials = n_trials or self.config.default_n_trials
        timeout = timeout or self.config.default_timeout_per_model
        
        print("\nüí°======================================================================")
        print("üí° OPTIMIZANDO LIGHTGBM CON MEJORAS DE FASE 1 Y FASE 2")
        print("üí°======================================================================")
        
        # Seleccionar estrategia de optimizaci√≥n autom√°ticamente
        if self.strategy_selector:
            strategy_config = self.strategy_selector.select_strategy(
                n_trials=n_trials,
                timeout=timeout,
                problem_type=optimization_strategy
            )
            print(f"   üìã Estrategia seleccionada: {strategy_config}")
        else:
            strategy_config = {'sampler': 'tpe', 'pruner': 'median'}
        
        # Log inicio de optimizaci√≥n del modelo
        if self.logger:
            self.logger.log_model_optimization_start('lightgbm', n_trials, {
                'timeout': timeout,
                'cv_folds': self.cv_folds,
                'use_temporal_cv': use_temporal_cv,
                'strategy': strategy_config
            })
        
        model_start_time = time.time()
        
        # Crear sampler y pruner avanzados
        try:
            sampler = self.sampler_factory.create_sampler(
                strategy_config.get('sampler', 'tpe'),
                SAMPLER_CONFIG
            )
            pruner = self.pruner_factory.create_pruner(
                strategy_config.get('pruner', 'median'),
                PRUNER_CONFIG
            )
            print(f"   üéØ Sampler: {type(sampler).__name__}")
            print(f"   ‚úÇÔ∏è  Pruner: {type(pruner).__name__}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error creando sampler/pruner avanzados: {e}")
            sampler = None
            pruner = None
        
        # Obtener monitor de early stopping
        if self.adaptive_controller:
            early_stopping_monitor = self.adaptive_controller.get_monitor('lightgbm')
            early_stopping_monitor.reset()
        else:
            early_stopping_monitor = None
        
        def objective(trial):
            """Funci√≥n objetivo para LightGBM con mejoras de Fase 1 y Fase 2"""
            trial_start_time = time.time()
            
            # Verificar early stopping
            if early_stopping_monitor and early_stopping_monitor.stopped:
                raise optuna.TrialPruned()
            
            # Configuraci√≥n base con GPU Manager
            base_params = {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'random_state': self.random_state,
                'verbosity': -1,
            }
            
            # Configuraci√≥n GPU/CPU inteligente
            if self.gpu_manager:
                try:
                    gpu_config = self.gpu_manager.get_lightgbm_config(
                        fallback_to_cpu=self.config.fallback_to_cpu
                    )
                    base_params.update(gpu_config)
                except Exception as e:
                    if self.logger:
                        self.logger.log_warning(f"Error configurando GPU para LightGBM: {e}")
                    base_params.update({'device': 'cpu', 'n_jobs': -1})
            else:
                base_params.update({'device': 'cpu', 'n_jobs': -1})
            
            # Hiperpar√°metros a optimizar usando MODEL_CONFIG
            lgb_config = MODEL_CONFIG.lightgbm_params
            params = base_params.copy()
            
            for param_name, param_config in lgb_config.items():
                if param_config['type'] == 'int':
                    if 'step' in param_config:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], 
                            param_config['high'], step=param_config['step']
                        )
                    else:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], param_config['high']
                        )
                elif param_config['type'] == 'float':
                    params[param_name] = trial.suggest_float(
                        param_name, param_config['low'], 
                        param_config['high'], log=param_config.get('log', False)
                    )
                elif param_config['type'] == 'categorical':
                    params[param_name] = trial.suggest_categorical(
                        param_name, param_config['choices']
                    )
            
            # Log inicio del trial
            if self.logger:
                self.logger.log_trial_start(trial.number, 'lightgbm', params)
            
            try:
                # Crear modelo
                model = lgb.LGBMClassifier(**params)
                
                # Usar validaci√≥n cruzada temporal si est√° disponible
                if use_temporal_cv and self.temporal_validator and 'date' in self.X_train.columns:
                    # Preparar datos con columna de fecha
                    X_train_with_date = self.X_train.copy()
                    if 'date' not in X_train_with_date.columns:
                        X_train_with_date['date'] = pd.date_range(start='2020-01-01', periods=len(X_train_with_date), freq='D')
                    
                    try:
                        cv_results = self.temporal_validator.perform_time_series_cv(
                            estimator=model,
                            X=X_train_with_date,
                            y=self.y_train,
                            scoring='roc_auc',
                            cv_type='time_series'
                        )
                        
                        cv_scores = cv_results['scores']
                        primary_score = cv_results['mean_score']
                        
                        # Log m√©tricas de estabilidad temporal
                        if self.logger:
                            self.logger.log_info(f"CV temporal completado - Trial {trial.number}", {
                                'mean_score': primary_score,
                                'std_score': cv_results['std_score'],
                                'stability_score': cv_results['stability_metrics']['stability_score'],
                                'n_folds': cv_results['n_folds']
                            })
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error en CV temporal, usando CV est√°ndar: {e}")
                        # Fallback a CV est√°ndar
                        cv_scores = cross_val_score(
                            model, self.X_train, self.y_train,
                            cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                            scoring='roc_auc',
                            n_jobs=-1
                        )
                        primary_score = cv_scores.mean()
                
                else:
                    # Validaci√≥n cruzada est√°ndar
                    cv_scores = cross_val_score(
                        model, self.X_train, self.y_train,
                        cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                        scoring='roc_auc',
                        n_jobs=-1
                    )
                    primary_score = cv_scores.mean()
                
                # Verificar early stopping inteligente
                if early_stopping_monitor:
                    should_stop = early_stopping_monitor.update(trial.number, primary_score)
                    if should_stop:
                        raise optuna.TrialPruned()
                
                # Calcular m√©tricas m√∫ltiples si est√° disponible
                if self.metrics_calculator:
                    try:
                        # Entrenar en datos completos para evaluaci√≥n
                        model.fit(self.X_train, self.y_train)
                        y_pred = model.predict(self.X_val)
                        y_proba = model.predict_proba(self.X_val)[:, 1]
                        
                        # Calcular todas las m√©tricas
                        metrics_result = self.metrics_calculator.calculate_all_metrics(
                            y_true=self.y_val.values,
                            y_pred=y_pred,
                            y_proba=y_proba,
                            cv_scores=cv_scores.tolist(),
                            metrics_to_calculate=self.config.secondary_metrics
                        )
                        
                        # Log m√©tricas
                        if self.logger:
                            self.logger.log_metrics(trial.number, 'lightgbm', metrics_result.secondary_scores)
                        
                        # Report intermediate value para pruning
                        trial.report(primary_score, trial.number)
                        
                        # Verificar si debe ser podado
                        if trial.should_prune():
                            if self.logger:
                                self.logger.log_trial_pruned(trial.number, 'lightgbm', "Optuna pruning")
                            raise optuna.TrialPruned()
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error calculando m√©tricas m√∫ltiples: {e}")
                
                # Log trial exitoso
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'lightgbm', primary_score, trial_duration)
                
                return primary_score
                
            except optuna.TrialPruned:
                # Re-raise pruned trials
                raise
            except Exception as e:
                # Log trial fallido
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'lightgbm', 0.0, trial_duration, "failed")
                    self.logger.log_error(f"Error en trial LightGBM {trial.number}", {'params': params}, e)
                
                raise optuna.TrialPruned()
        
        # Crear y ejecutar estudio con configuraci√≥n avanzada
        study_name = f"lightgbm_phase2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        study_kwargs = {
            'direction': 'maximize',
            'study_name': study_name,
            'storage': f'sqlite:///{self.results_path}/optuna_studies.db',
            'load_if_exists': True
        }
        
        # Agregar sampler y pruner si est√°n disponibles
        if sampler:
            study_kwargs['sampler'] = sampler
        if pruner:
            study_kwargs['pruner'] = pruner
        
        study = optuna.create_study(**study_kwargs)
        
        print(f"   üéØ Ejecutando {n_trials} trials con estrategia '{optimization_strategy}'...")
        if timeout:
            print(f"   ‚è∞ Timeout: {timeout} segundos")
        
        # Callback para progreso con early stopping
        def progress_callback(study, trial):
            if trial.number % 10 == 0:
                current_best = study.best_value if study.best_value else 0.0
                
                if self.logger:
                    self.logger.log_progress(trial.number, n_trials, current_best, 'lightgbm')
                
                # Verificar early stopping global
                if early_stopping_monitor:
                    should_stop = self.adaptive_controller.should_stop_model(
                        'lightgbm', trial.number, current_best
                    )
                    if should_stop:
                        study.stop()
        
        # Ejecutar optimizaci√≥n
        study.optimize(
            objective, 
            n_trials=n_trials, 
            timeout=timeout, 
            callbacks=[progress_callback],
            catch=(Exception,)  # Capturar excepciones para continuar
        )
        
        # Guardar resultados
        model_duration = time.time() - model_start_time
        self.studies['lightgbm'] = study
        self.best_params['lightgbm'] = study.best_params
        self.best_scores['lightgbm'] = study.best_value
        
        # Guardar historial de convergencia
        if early_stopping_monitor:
            self.convergence_history['lightgbm'] = early_stopping_monitor.get_summary()
        
        print(f"   ‚úÖ Optimizaci√≥n completada!")
        print(f"   üèÜ Mejor AUC: {study.best_value:.4f}")
        print(f"   üîß Mejores par√°metros: {study.best_params}")
        print(f"   ‚è∞ Tiempo total: {model_duration:.1f}s")
        print(f"   üéØ Trials ejecutados: {len(study.trials)}")
        
        # Informaci√≥n de convergencia
        if early_stopping_monitor:
            convergence_info = early_stopping_monitor.get_summary()
            print(f"   üìä Early stopping: {convergence_info['stopped']}")
            if convergence_info['stopped']:
                print(f"   üõë Raz√≥n: {convergence_info['stop_reason']}")
        
        # Log finalizaci√≥n
        if self.logger:
            self.logger.log_model_optimization_complete(
                'lightgbm', study.best_value, study.best_params, model_duration
            )
        
        return study
    
    def optimize_catboost(self, n_trials: int = None, timeout: Optional[int] = None,
                         use_temporal_cv: bool = True, optimization_strategy: str = 'balanced'):
        """
        Optimizar hiperpar√°metros de CatBoost con mejoras de Fase 1 y Fase 2
        
        Args:
            n_trials: N√∫mero de trials (None para usar estrategia)
            timeout: Timeout en segundos (None para usar estrategia)
            use_temporal_cv: Usar validaci√≥n cruzada temporal
            optimization_strategy: Estrategia de optimizaci√≥n ('quick', 'balanced', 'thorough')
        """
        n_trials = n_trials or self.config.default_n_trials
        timeout = timeout or self.config.default_timeout_per_model
        
        print("\nüê±======================================================================")
        print("üê± OPTIMIZANDO CATBOOST CON MEJORAS DE FASE 1 Y FASE 2")
        print("üê±======================================================================")
        
        # Seleccionar estrategia de optimizaci√≥n autom√°ticamente
        if self.strategy_selector:
            strategy_config = self.strategy_selector.select_strategy(
                n_trials=n_trials,
                timeout=timeout,
                problem_type=optimization_strategy
            )
            print(f"   üìã Estrategia seleccionada: {strategy_config}")
        else:
            strategy_config = {'sampler': 'tpe', 'pruner': 'median'}
        
        # Log inicio de optimizaci√≥n del modelo
        if self.logger:
            self.logger.log_model_optimization_start('catboost', n_trials, {
                'timeout': timeout,
                'cv_folds': self.cv_folds,
                'use_temporal_cv': use_temporal_cv,
                'strategy': strategy_config
            })
        
        model_start_time = time.time()
        
        # Crear sampler y pruner avanzados
        try:
            sampler = self.sampler_factory.create_sampler(
                strategy_config.get('sampler', 'tpe'),
                SAMPLER_CONFIG
            )
            pruner = self.pruner_factory.create_pruner(
                strategy_config.get('pruner', 'median'),
                PRUNER_CONFIG
            )
            print(f"   üéØ Sampler: {type(sampler).__name__}")
            print(f"   ‚úÇÔ∏è  Pruner: {type(pruner).__name__}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error creando sampler/pruner avanzados: {e}")
            sampler = None
            pruner = None
        
        # Obtener monitor de early stopping
        if self.adaptive_controller:
            early_stopping_monitor = self.adaptive_controller.get_monitor('catboost')
            early_stopping_monitor.reset()
        else:
            early_stopping_monitor = None
        
        def objective(trial):
            """Funci√≥n objetivo para CatBoost con mejoras de Fase 1 y Fase 2"""
            trial_start_time = time.time()
            
            # Verificar early stopping
            if early_stopping_monitor and early_stopping_monitor.stopped:
                raise optuna.TrialPruned()
            
            # Configuraci√≥n base con GPU Manager
            base_params = {
                'objective': 'Logloss',
                'eval_metric': 'AUC',
                'random_state': self.random_state,
                'verbose': False,
                'allow_writing_files': False,
            }
            
            # Configuraci√≥n GPU/CPU inteligente
            if self.gpu_manager:
                try:
                    gpu_config = self.gpu_manager.get_catboost_config(
                        fallback_to_cpu=self.config.fallback_to_cpu
                    )
                    base_params.update(gpu_config)
                except Exception as e:
                    if self.logger:
                        self.logger.log_warning(f"Error configurando GPU para CatBoost: {e}")
                    base_params.update({'task_type': 'CPU', 'thread_count': -1})
            else:
                base_params.update({'task_type': 'CPU', 'thread_count': -1})
            
            # Hiperpar√°metros a optimizar usando MODEL_CONFIG
            cb_config = MODEL_CONFIG.catboost_params
            params = base_params.copy()
            
            for param_name, param_config in cb_config.items():
                if param_config['type'] == 'int':
                    if 'step' in param_config:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], 
                            param_config['high'], step=param_config['step']
                        )
                    else:
                        params[param_name] = trial.suggest_int(
                            param_name, param_config['low'], param_config['high']
                        )
                elif param_config['type'] == 'float':
                    params[param_name] = trial.suggest_float(
                        param_name, param_config['low'], 
                        param_config['high'], log=param_config.get('log', False)
                    )
                elif param_config['type'] == 'categorical':
                    params[param_name] = trial.suggest_categorical(
                        param_name, param_config['choices']
                    )
            
            # Par√°metros espec√≠ficos para bootstrap
            if params.get('bootstrap_type') == 'Bayesian':
                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)
            elif params.get('bootstrap_type') == 'Bernoulli':
                params['subsample'] = trial.suggest_float('subsample_bernoulli', 0.6, 1.0)
            
            # Log inicio del trial
            if self.logger:
                self.logger.log_trial_start(trial.number, 'catboost', params)
            
            try:
                # Crear modelo
                model = cb.CatBoostClassifier(**params)
                
                # Usar validaci√≥n cruzada temporal si est√° disponible
                if use_temporal_cv and self.temporal_validator and 'date' in self.X_train.columns:
                    # Preparar datos con columna de fecha
                    X_train_with_date = self.X_train.copy()
                    if 'date' not in X_train_with_date.columns:
                        X_train_with_date['date'] = pd.date_range(start='2020-01-01', periods=len(X_train_with_date), freq='D')
                    
                    try:
                        cv_results = self.temporal_validator.perform_time_series_cv(
                            estimator=model,
                            X=X_train_with_date,
                            y=self.y_train,
                            scoring='roc_auc',
                            cv_type='time_series'
                        )
                        
                        cv_scores = cv_results['scores']
                        primary_score = cv_results['mean_score']
                        
                        # Log m√©tricas de estabilidad temporal
                        if self.logger:
                            self.logger.log_info(f"CV temporal completado - Trial {trial.number}", {
                                'mean_score': primary_score,
                                'std_score': cv_results['std_score'],
                                'stability_score': cv_results['stability_metrics']['stability_score'],
                                'n_folds': cv_results['n_folds']
                            })
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error en CV temporal, usando CV est√°ndar: {e}")
                        # Fallback a CV est√°ndar
                        cv_scores = cross_val_score(
                            model, self.X_train, self.y_train,
                            cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                            scoring='roc_auc',
                            n_jobs=-1
                        )
                        primary_score = cv_scores.mean()
                
                else:
                    # Validaci√≥n cruzada est√°ndar
                    cv_scores = cross_val_score(
                        model, self.X_train, self.y_train,
                        cv=StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state),
                        scoring='roc_auc',
                        n_jobs=-1
                    )
                    primary_score = cv_scores.mean()
                
                # Verificar early stopping inteligente
                if early_stopping_monitor:
                    should_stop = early_stopping_monitor.update(trial.number, primary_score)
                    if should_stop:
                        raise optuna.TrialPruned()
                
                # Calcular m√©tricas m√∫ltiples si est√° disponible
                if self.metrics_calculator:
                    try:
                        # Entrenar en datos completos para evaluaci√≥n
                        model.fit(self.X_train, self.y_train)
                        y_pred = model.predict(self.X_val)
                        y_proba = model.predict_proba(self.X_val)[:, 1]
                        
                        # Calcular todas las m√©tricas
                        metrics_result = self.metrics_calculator.calculate_all_metrics(
                            y_true=self.y_val.values,
                            y_pred=y_pred,
                            y_proba=y_proba,
                            cv_scores=cv_scores.tolist(),
                            metrics_to_calculate=self.config.secondary_metrics
                        )
                        
                        # Log m√©tricas
                        if self.logger:
                            self.logger.log_metrics(trial.number, 'catboost', metrics_result.secondary_scores)
                        
                        # Report intermediate value para pruning
                        trial.report(primary_score, trial.number)
                        
                        # Verificar si debe ser podado
                        if trial.should_prune():
                            if self.logger:
                                self.logger.log_trial_pruned(trial.number, 'catboost', "Optuna pruning")
                            raise optuna.TrialPruned()
                        
                    except Exception as e:
                        if self.logger:
                            self.logger.log_warning(f"Error calculando m√©tricas m√∫ltiples: {e}")
                
                # Log trial exitoso
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'catboost', primary_score, trial_duration)
                
                return primary_score
                
            except optuna.TrialPruned:
                # Re-raise pruned trials
                raise
            except Exception as e:
                # Log trial fallido
                trial_duration = time.time() - trial_start_time
                if self.logger:
                    self.logger.log_trial_complete(trial.number, 'catboost', 0.0, trial_duration, "failed")
                    self.logger.log_error(f"Error en trial CatBoost {trial.number}", {'params': params}, e)
                
                raise optuna.TrialPruned()
        
        # Crear y ejecutar estudio con configuraci√≥n avanzada
        study_name = f"catboost_phase2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        study_kwargs = {
            'direction': 'maximize',
            'study_name': study_name,
            'storage': f'sqlite:///{self.results_path}/optuna_studies.db',
            'load_if_exists': True
        }
        
        # Agregar sampler y pruner si est√°n disponibles
        if sampler:
            study_kwargs['sampler'] = sampler
        if pruner:
            study_kwargs['pruner'] = pruner
        
        study = optuna.create_study(**study_kwargs)
        
        print(f"   üéØ Ejecutando {n_trials} trials con estrategia '{optimization_strategy}'...")
        if timeout:
            print(f"   ‚è∞ Timeout: {timeout} segundos")
        
        # Callback para progreso con early stopping
        def progress_callback(study, trial):
            if trial.number % 10 == 0:
                current_best = study.best_value if study.best_value else 0.0
                
                if self.logger:
                    self.logger.log_progress(trial.number, n_trials, current_best, 'catboost')
                
                # Verificar early stopping global
                if early_stopping_monitor:
                    should_stop = self.adaptive_controller.should_stop_model(
                        'catboost', trial.number, current_best
                    )
                    if should_stop:
                        study.stop()
        
        # Ejecutar optimizaci√≥n
        study.optimize(
            objective, 
            n_trials=n_trials, 
            timeout=timeout, 
            callbacks=[progress_callback],
            catch=(Exception,)  # Capturar excepciones para continuar
        )
        
        # Guardar resultados
        model_duration = time.time() - model_start_time
        self.studies['catboost'] = study
        self.best_params['catboost'] = study.best_params
        self.best_scores['catboost'] = study.best_value
        
        # Guardar historial de convergencia
        if early_stopping_monitor:
            self.convergence_history['catboost'] = early_stopping_monitor.get_summary()
        
        print(f"   ‚úÖ Optimizaci√≥n completada!")
        print(f"   üèÜ Mejor AUC: {study.best_value:.4f}")
        print(f"   üîß Mejores par√°metros: {study.best_params}")
        print(f"   ‚è∞ Tiempo total: {model_duration:.1f}s")
        print(f"   üéØ Trials ejecutados: {len(study.trials)}")
        
        # Informaci√≥n de convergencia
        if early_stopping_monitor:
            convergence_info = early_stopping_monitor.get_summary()
            print(f"   üìä Early stopping: {convergence_info['stopped']}")
            if convergence_info['stopped']:
                print(f"   üõë Raz√≥n: {convergence_info['stop_reason']}")
        
        # Log finalizaci√≥n
        if self.logger:
            self.logger.log_model_optimization_complete(
                'catboost', study.best_value, study.best_params, model_duration
            )
        
        return study
    
    def optimize_all_models(self, n_trials: int = None, timeout_per_model: Optional[int] = None,
                           use_temporal_cv: bool = True, optimization_strategy: str = 'balanced'):
        """
        Optimizar todos los modelos secuencialmente con mejoras de Fase 1 y Fase 2
        
        Args:
            n_trials: N√∫mero de trials por modelo (None para usar estrategia)
            timeout_per_model: Timeout por modelo en segundos (None para usar estrategia)
            use_temporal_cv: Usar validaci√≥n cruzada temporal
            optimization_strategy: Estrategia de optimizaci√≥n ('quick', 'balanced', 'thorough')
        """
        n_trials = n_trials or self.config.default_n_trials
        timeout_per_model = timeout_per_model or self.config.default_timeout_per_model
        
        print("üöÄ======================================================================")
        print("üöÄ OPTIMIZACI√ìN COMPLETA DE TODOS LOS MODELOS CON FASE 1 Y FASE 2")
        print("üöÄ======================================================================")
        print(f"   üéØ Estrategia: {optimization_strategy}")
        print(f"   üî¢ Trials por modelo: {n_trials}")
        print(f"   ‚è∞ Timeout por modelo: {timeout_per_model}s")
        print(f"   üìÖ Validaci√≥n temporal: {use_temporal_cv}")
        
        # Lista de modelos a optimizar
        models_to_optimize = [
            ('XGBoost', self.optimize_xgboost),
            ('LightGBM', self.optimize_lightgbm),
            ('CatBoost', self.optimize_catboost)
        ]
        
        start_time = datetime.now()
        
        # Log inicio de optimizaci√≥n completa
        if self.logger:
            self.logger.log_optimization_start({
                'models': [name for name, _ in models_to_optimize],
                'n_trials': n_trials,
                'timeout_per_model': timeout_per_model,
                'use_temporal_cv': use_temporal_cv,
                'optimization_strategy': optimization_strategy,
                'phase_1_enabled': self.data_validator is not None,
                'phase_2_enabled': self.temporal_validator is not None
            })
        
        optimization_results = {}
        
        for model_name, optimize_func in models_to_optimize:
            print(f"\nüéØ Iniciando optimizaci√≥n de {model_name}...")
            model_start = datetime.now()
            
            try:
                # Ejecutar optimizaci√≥n con nuevos par√°metros
                study = optimize_func(
                    n_trials=n_trials, 
                    timeout=timeout_per_model,
                    use_temporal_cv=use_temporal_cv,
                    optimization_strategy=optimization_strategy
                )
                
                model_time = datetime.now() - model_start
                print(f"   ‚è∞ Tiempo {model_name}: {model_time}")
                
                # Guardar resultados detallados
                optimization_results[model_name] = {
                    'best_score': study.best_value,
                    'best_params': study.best_params,
                    'n_trials': len(study.trials),
                    'duration': model_time.total_seconds(),
                    'convergence_info': self.convergence_history.get(model_name.lower(), {})
                }
                
            except Exception as e:
                print(f"   ‚ùå Error optimizando {model_name}: {e}")
                if self.logger:
                    self.logger.log_error(f"Error en optimizaci√≥n de {model_name}", {}, e)
                
                optimization_results[model_name] = {
                    'error': str(e),
                    'duration': (datetime.now() - model_start).total_seconds()
                }
                continue
        
        total_time = datetime.now() - start_time
        print(f"\n‚è∞ Tiempo total de optimizaci√≥n: {total_time}")
        
        # Mostrar resumen de resultados
        print("\nüìä RESUMEN DE OPTIMIZACI√ìN:")
        print("="*60)
        for model_name, results in optimization_results.items():
            if 'error' not in results:
                print(f"   üèÜ {model_name.upper()}: AUC = {results['best_score']:.4f} "
                      f"({results['n_trials']} trials, {results['duration']:.1f}s)")
                
                # Informaci√≥n de convergencia
                if results['convergence_info']:
                    conv_info = results['convergence_info']
                    if conv_info.get('stopped'):
                        print(f"      üõë Early stopping: {conv_info.get('stop_reason', 'Unknown')}")
            else:
                print(f"   ‚ùå {model_name.upper()}: Error - {results['error']}")
        
        # Log finalizaci√≥n
        if self.logger:
            self.logger.log_optimization_complete({
                'total_duration': total_time.total_seconds(),
                'results': optimization_results,
                'best_overall': max(
                    [(k, v['best_score']) for k, v in optimization_results.items() 
                     if 'best_score' in v], 
                    key=lambda x: x[1], default=('none', 0.0)
                )
            })
        
        # Guardar resumen de resultados
        self.save_optimization_summary()
        
        return self
    
    def evaluate_best_models(self):
        """
        Evaluar los mejores modelos encontrados en el conjunto de validaci√≥n
        """
        print("\nüìä======================================================================")
        print("üìä EVALUANDO MEJORES MODELOS EN VALIDACI√ìN")
        print("üìä======================================================================")
        
        evaluation_results = {}
        
        for model_name in self.best_params.keys():
            print(f"\nüîç Evaluando {model_name.upper()}...")
            
            try:
                # Crear modelo con mejores par√°metros
                if model_name == 'xgboost':
                    model = xgb.XGBClassifier(**self.best_params[model_name])
                elif model_name == 'lightgbm':
                    model = lgb.LGBMClassifier(**self.best_params[model_name])
                elif model_name == 'catboost':
                    model = cb.CatBoostClassifier(**self.best_params[model_name])
                
                # Entrenar en datos de entrenamiento
                model.fit(self.X_train, self.y_train)
                
                # Evaluar en validaci√≥n
                y_pred_proba = model.predict_proba(self.X_val)[:, 1]
                val_auc = roc_auc_score(self.y_val, y_pred_proba)
                
                # Evaluar en test
                y_test_pred_proba = model.predict_proba(self.X_test)[:, 1]
                test_auc = roc_auc_score(self.y_test, y_test_pred_proba)
                
                evaluation_results[model_name] = {
                    'cv_score': self.best_scores[model_name],
                    'val_auc': val_auc,
                    'test_auc': test_auc,
                    'params': self.best_params[model_name]
                }
                
                print(f"   üìä CV Score: {self.best_scores[model_name]:.4f}")
                print(f"   üìä Validation AUC: {val_auc:.4f}")
                print(f"   üìä Test AUC: {test_auc:.4f}")
                
            except Exception as e:
                print(f"   ‚ùå Error evaluando {model_name}: {e}")
                continue
        
        # Guardar resultados de evaluaci√≥n
        results_file = self.results_path / f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w') as f:
            json.dump(evaluation_results, f, indent=2, default=str)
        
        print(f"\nüíæ Resultados guardados en: {results_file}")
        
        return evaluation_results
    
    def save_optimization_summary(self):
        """
        Guardar resumen completo de la optimizaci√≥n
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Resumen de mejores par√°metros
        summary = {
            'timestamp': timestamp,
            'best_params': self.best_params,
            'best_scores': self.best_scores,
            'cv_folds': self.cv_folds,
            'random_state': self.random_state
        }
        
        # Guardar como JSON
        summary_file = self.results_path / f"optimization_summary_{timestamp}.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        # Guardar estudios como pickle
        studies_file = self.results_path / f"optuna_studies_{timestamp}.pkl"
        with open(studies_file, 'wb') as f:
            pickle.dump(self.studies, f)
        
        print(f"\nüíæ Resumen guardado en: {summary_file}")
        print(f"üíæ Estudios guardados en: {studies_file}")
        
        return summary_file, studies_file
    
    def generate_visualizations(self):
        """
        Generar visualizaciones de los estudios de optimizaci√≥n
        """
        print("\nüìà======================================================================")
        print("üìà GENERANDO VISUALIZACIONES")
        print("üìà======================================================================")
        
        import matplotlib.pyplot as plt
        plt.style.use('seaborn-v0_8')
        
        for model_name, study in self.studies.items():
            print(f"\nüìä Generando gr√°ficos para {model_name}...")
            
            # Crear directorio para visualizaciones
            viz_dir = self.results_path / "visualizations" / model_name
            viz_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                # 1. Historia de optimizaci√≥n
                fig1 = plot_optimization_history(study)
                fig1.write_html(viz_dir / "optimization_history.html")
                
                # 2. Importancia de par√°metros
                fig2 = plot_param_importances(study)
                fig2.write_html(viz_dir / "param_importances.html")
                
                # 3. Gr√°fico de contorno (solo para los 2 par√°metros m√°s importantes)
                if len(study.best_params) >= 2:
                    param_names = list(study.best_params.keys())[:2]
                    fig3 = plot_contour(study, params=param_names)
                    fig3.write_html(viz_dir / f"contour_{param_names[0]}_{param_names[1]}.html")
                
                print(f"   ‚úÖ Visualizaciones guardadas en: {viz_dir}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error generando visualizaciones para {model_name}: {e}")
    
    def print_final_summary(self):
        """
        Imprimir resumen final de la optimizaci√≥n
        """
        print("\nüèÜ======================================================================")
        print("üèÜ RESUMEN FINAL DE OPTIMIZACI√ìN")
        print("üèÜ======================================================================")
        
        if not self.best_scores:
            print("   ‚ùå No hay resultados de optimizaci√≥n disponibles")
            return
        
        # Ordenar modelos por performance
        sorted_models = sorted(self.best_scores.items(), key=lambda x: x[1], reverse=True)
        
        print("\nüèÖ RANKING DE MODELOS (por CV Score):")
        for i, (model_name, score) in enumerate(sorted_models, 1):
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
            print(f"   {medal} {i}. {model_name.upper():12} AUC: {score:.4f}")
        
        print("\nüîß MEJORES HIPERPAR√ÅMETROS:")
        for model_name in self.best_params:
            print(f"\n   üîπ {model_name.upper()}:")
            for param, value in self.best_params[model_name].items():
                print(f"      {param}: {value}")
        
        print(f"\nüìÅ Resultados guardados en: {self.results_path}")
    
    def __del__(self):
        """Destructor para limpiar recursos de Fase 3"""
        try:
            if hasattr(self, 'memory_manager') and self.memory_manager:
                self.memory_manager.stop()
                
            if hasattr(self, 'worker_manager') and self.worker_manager:
                self.worker_manager.stop_workers()
                
            if hasattr(self, 'logger') and self.logger:
                self.logger.log_info("CryptoHyperparameterOptimizer destruido")
        except:
            pass
    
    def cleanup_resources(self):
        """Limpiar recursos expl√≠citamente"""
        try:
            if self.memory_manager:
                self.memory_manager.stop()
                print("üß† Gesti√≥n de memoria detenida")
                
            if self.worker_manager:
                self.worker_manager.stop_workers()
                print("üë• Workers detenidos")
                
            if self.logger:
                self.logger.log_info("Recursos limpiados exitosamente")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error limpiando recursos: {e}")
            # Log el error tambi√©n si el logger funciona
            if hasattr(self, 'logger') and self.logger:
                try:
                    self.logger.log_error(f"Error limpiando recursos: {e}", exception=e)
                except:
                    pass  # Si el logger tambi√©n falla, no hacer nada
    
    def get_system_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas del sistema de optimizaci√≥n"""
        stats = {
            'phase_1_enabled': self.data_validator is not None,
            'phase_2_enabled': self.temporal_validator is not None,
            'phase_3_enabled': self.worker_manager is not None and self.memory_manager is not None,
            'timestamp': time.time()
        }
        
        # Estad√≠sticas de Fase 3
        if self.memory_manager:
            stats['memory_stats'] = self.memory_manager.get_comprehensive_stats()
            
        if self.worker_manager:
            stats['worker_stats'] = self.worker_manager.get_stats()
            
        if self.parallel_trial_executor:
            stats['parallel_stats'] = self.parallel_trial_executor.get_performance_metrics()
            
        return stats

    def optimize_all_models_parallel(self, n_trials: int = None, timeout_per_model: Optional[int] = None,
                                    use_temporal_cv: bool = True, optimization_strategy: str = 'balanced',
                                    enable_parallelization: bool = True, 
                                    enable_memory_optimization: bool = True) -> Dict[str, Any]:
        """
        Optimizar todos los modelos con paralelizaci√≥n y gesti√≥n de memoria (Fase 3)
        
        Args:
            n_trials: N√∫mero de trials por modelo
            timeout_per_model: Timeout por modelo en segundos
            use_temporal_cv: Usar validaci√≥n cruzada temporal
            optimization_strategy: Estrategia de optimizaci√≥n
            enable_parallelization: Habilitar paralelizaci√≥n
            enable_memory_optimization: Habilitar gesti√≥n de memoria
        """
        n_trials = n_trials or self.config.default_n_trials
        timeout_per_model = timeout_per_model or self.config.default_timeout_per_model
        
        print("üöÄ======================================================================")
        print("üöÄ OPTIMIZACI√ìN PARALELA COMPLETA - FASE 3")
        print("üöÄ======================================================================")
        print(f"   üéØ Estrategia: {optimization_strategy}")
        print(f"   üî¢ Trials por modelo: {n_trials}")
        print(f"   ‚è∞ Timeout por modelo: {timeout_per_model}s")
        print(f"   üìÖ Validaci√≥n temporal: {use_temporal_cv}")
        print(f"   üë• Paralelizaci√≥n: {'‚úÖ' if enable_parallelization and self.worker_manager else '‚ùå'}")
        print(f"   üß† Gesti√≥n memoria: {'‚úÖ' if enable_memory_optimization and self.memory_manager else '‚ùå'}")
        
        start_time = datetime.now()
        
        # Inicializar componentes de Fase 3 si est√°n habilitados
        if enable_parallelization and self.worker_manager:
            self.worker_manager.start_workers()
            print(f"   üë• Workers iniciados: {self.parallelization_config.n_workers}")
        
        if enable_memory_optimization and self.memory_manager:
            # Optimizaci√≥n inicial de memoria
            memory_opt_result = self.memory_manager.optimize_memory()
            print(f"   üß† Memoria optimizada: {memory_opt_result['gc_result']['memory_freed']:.1f}MB liberados")
        
        # Lista de modelos a optimizar
        models_to_optimize = [
            ('XGBoost', self.optimize_xgboost),
            ('LightGBM', self.optimize_lightgbm),
            ('CatBoost', self.optimize_catboost)
        ]
        
        # Log inicio de optimizaci√≥n
        if self.logger:
            self.logger.log_optimization_start({
                'models': [name for name, _ in models_to_optimize],
                'n_trials': n_trials,
                'timeout_per_model': timeout_per_model,
                'use_temporal_cv': use_temporal_cv,
                'optimization_strategy': optimization_strategy,
                'phase_1_enabled': self.data_validator is not None,
                'phase_2_enabled': self.temporal_validator is not None,
                'phase_3_enabled': enable_parallelization or enable_memory_optimization,
                'parallelization_enabled': enable_parallelization,
                'memory_optimization_enabled': enable_memory_optimization
            })
        
        optimization_results = {}
        
        # Optimizaci√≥n secuencial con mejoras de Fase 3
        for i, (model_name, optimize_func) in enumerate(models_to_optimize):
            print(f"\nüéØ Iniciando optimizaci√≥n de {model_name} ({i+1}/{len(models_to_optimize)})...")
            model_start = datetime.now()
            
            try:
                # Gesti√≥n de memoria antes de cada modelo
                if enable_memory_optimization and self.memory_manager:
                    if i > 0:  # No en el primer modelo
                        gc_result = self.memory_manager.gc_manager.auto_gc_if_needed()
                        if gc_result:
                            print(f"   üß† GC autom√°tico: {gc_result['memory_freed']:.1f}MB liberados")
                
                # Ejecutar optimizaci√≥n (con paralelizaci√≥n interna de Optuna si est√° habilitada)
                if enable_parallelization and self.parallel_trial_executor:
                    # Usar paralelizaci√≥n avanzada
                    print(f"   üë• Usando optimizaci√≥n paralela con {self.parallelization_config.n_workers} workers")
                    
                result = optimize_func(
                    n_trials=n_trials,
                    timeout=timeout_per_model,
                    use_temporal_cv=use_temporal_cv,
                    optimization_strategy=optimization_strategy
                )
                
                # Guardar resultado
                optimization_results[model_name] = result
                
                # Log progreso
                model_duration = (datetime.now() - model_start).total_seconds()
                print(f"   ‚úÖ {model_name} completado en {model_duration:.1f}s")
                
                if self.logger:
                    self.logger.log_model_completion(model_name, result, model_duration)
                
                # Estad√≠sticas de memoria despu√©s del modelo
                if enable_memory_optimization and self.memory_manager:
                    memory_stats = self.memory_manager.monitor.get_current_stats()
                    print(f"   üìä Memoria: {memory_stats['used_percent']:.1f}% "
                          f"({memory_stats['used_mb']:.0f}MB)")
                
            except Exception as e:
                print(f"   ‚ùå Error optimizando {model_name}: {e}")
                optimization_results[model_name] = {'error': str(e)}
                
                if self.logger:
                    self.logger.error(f"Error optimizando {model_name}: {e}")
        
        # Finalizaci√≥n
        total_duration = (datetime.now() - start_time).total_seconds()
        
        # Resumen final
        print(f"\nüèÅ Optimizaci√≥n completa en {total_duration:.1f}s")
        
        successful_models = [name for name, result in optimization_results.items() 
                           if 'error' not in result]
        failed_models = [name for name, result in optimization_results.items() 
                        if 'error' in result]
        
        print(f"   ‚úÖ Modelos exitosos: {len(successful_models)}")
        print(f"   ‚ùå Modelos fallidos: {len(failed_models)}")
        
        # Estad√≠sticas finales de Fase 3
        final_stats = {}
        if enable_memory_optimization and self.memory_manager:
            final_stats['memory'] = self.memory_manager.get_comprehensive_stats()
            print(f"   üß† Memoria final: {final_stats['memory']['memory_stats']['used_percent']:.1f}%")
        
        if enable_parallelization and self.worker_manager:
            final_stats['workers'] = self.worker_manager.get_stats()
            print(f"   üë• Workers utilizados: {final_stats['workers']['n_workers']}")
        
        # Crear resumen completo
        summary = {
            'optimization_results': optimization_results,
            'total_duration': total_duration,
            'successful_models': successful_models,
            'failed_models': failed_models,
            'phase_3_stats': final_stats,
            'configuration': {
                'n_trials': n_trials,
                'timeout_per_model': timeout_per_model,
                'use_temporal_cv': use_temporal_cv,
                'optimization_strategy': optimization_strategy,
                'parallelization_enabled': enable_parallelization,
                'memory_optimization_enabled': enable_memory_optimization
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # Guardar resumen
        if self.logger:
            self.logger.log_optimization_summary(summary)
        
        # Limpiar workers al final
        if enable_parallelization and self.worker_manager:
            self.worker_manager.stop_workers()
            print("   üë• Workers detenidos")
        
        return summary

def main():
    """
    Funci√≥n principal para ejecutar optimizaci√≥n completa con mejoras de Fase 1
    """
    print("üöÄ SISTEMA DE OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS - FASE 1")
    print("üöÄ CRIPTOMONEDAS DE BAJA CAPITALIZACI√ìN")
    print("üöÄ MEJORAS: Validaci√≥n robusta, GPU inteligente, m√©tricas m√∫ltiples, logging")
    print("üöÄ======================================================================")
    
    # Inicializar optimizador con configuraci√≥n mejorada
    try:
        optimizer = CryptoHyperparameterOptimizer()
        
        # Cargar y preparar datos con validaci√≥n robusta
        optimizer.load_and_prepare_data()
        
        # Configuraci√≥n de optimizaci√≥n desde CONFIG
        N_TRIALS = optimizer.config.default_n_trials
        TIMEOUT_PER_MODEL = optimizer.config.default_timeout_per_model
        
        print(f"\n‚öôÔ∏è  CONFIGURACI√ìN DE OPTIMIZACI√ìN (FASE 1):")
        print(f"   üî¢ Trials por modelo: {N_TRIALS}")
        print(f"   ‚è∞ Timeout por modelo: {TIMEOUT_PER_MODEL} segundos")
        print(f"   üîÑ CV folds: {optimizer.cv_folds}")
        print(f"   üéØ M√©trica primaria: {optimizer.config.primary_metric}")
        print(f"   üìä M√©tricas secundarias: {len(optimizer.config.secondary_metrics)}")
        print(f"   üéÆ GPU disponible: {'‚úÖ' if optimizer.gpu_manager and optimizer.gpu_manager.cuda_available else '‚ùå'}")
        
        # Ejecutar optimizaci√≥n completa
        optimizer.optimize_all_models(
            n_trials=N_TRIALS,
            timeout_per_model=TIMEOUT_PER_MODEL
        )
        
        # Evaluar mejores modelos
        optimizer.evaluate_best_models()
        
        # Generar visualizaciones
        optimizer.generate_visualizations()
        
        # Resumen final
        optimizer.print_final_summary()
        
        # Finalizar logging
        if optimizer.logger:
            optimizer.logger.log_optimization_complete({
                'best_scores': optimizer.best_scores,
                'best_params': optimizer.best_params
            })
        
        print("\n‚úÖ OPTIMIZACI√ìN FASE 1 COMPLETADA EXITOSAMENTE!")
        
    except Exception as e:
        print(f"\n‚ùå ERROR EN OPTIMIZACI√ìN: {e}")
        import traceback
        traceback.print_exc()
        
        # Log error si est√° disponible
        if 'optimizer' in locals() and hasattr(optimizer, 'logger') and optimizer.logger:
            optimizer.logger.log_error("Error cr√≠tico en optimizaci√≥n", exception=e)
        
        sys.exit(1)

if __name__ == "__main__":
    main()
