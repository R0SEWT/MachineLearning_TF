#!/usr/bin/env python3
"""
Analizador y visualizador de resultados de optimizaci√≥n Optuna
Genera reportes detallados y gr√°ficos interactivos
"""

import sys
import os
import pandas as pd
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# Agregar paths necesarios
sys.path.append('/home/exodia/Documentos/MachineLearning_TF/code/EDA/utils')

try:
    import optuna
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.io as pio
    PLOTTING_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Librer√≠as de visualizaci√≥n no disponibles")
    PLOTTING_AVAILABLE = False

class OptunaResultsAnalyzer:
    """
    Analizador completo de resultados de optimizaci√≥n Optuna
    """
    
    def __init__(self, results_path: str = "../../optimization_results"):
        """
        Inicializar analizador
        
        Args:
            results_path: Ruta donde est√°n los resultados
        """
        self.results_path = Path(results_path)
        self.studies = {}
        self.summaries = []
        self.evaluations = []
        
        print(f"üîç OptunaResultsAnalyzer inicializado")
        print(f"   üìÅ Buscando resultados en: {self.results_path}")
        
        self.load_all_results()
    
    def load_all_results(self):
        """
        Cargar todos los resultados disponibles
        """
        print("\nüìÇ Cargando resultados...")
        
        # Cargar estudios de Optuna
        study_files = list(self.results_path.glob("optuna_studies_*.pkl"))
        for study_file in study_files:
            try:
                with open(study_file, 'rb') as f:
                    studies = pickle.load(f)
                    timestamp = study_file.stem.split('_')[-2:]  # Extraer timestamp
                    self.studies[f"{'_'.join(timestamp)}"] = studies
                print(f"   ‚úÖ Estudios cargados: {study_file.name}")
            except Exception as e:
                print(f"   ‚ùå Error cargando {study_file.name}: {e}")
        
        # Cargar res√∫menes
        summary_files = list(self.results_path.glob("optimization_summary_*.json"))
        for summary_file in summary_files:
            try:
                with open(summary_file, 'r') as f:
                    summary = json.load(f)
                    self.summaries.append(summary)
                print(f"   ‚úÖ Resumen cargado: {summary_file.name}")
            except Exception as e:
                print(f"   ‚ùå Error cargando {summary_file.name}: {e}")
        
        # Cargar evaluaciones
        eval_files = list(self.results_path.glob("evaluation_results_*.json"))
        for eval_file in eval_files:
            try:
                with open(eval_file, 'r') as f:
                    evaluation = json.load(f)
                    self.evaluations.append(evaluation)
                print(f"   ‚úÖ Evaluaci√≥n cargada: {eval_file.name}")
            except Exception as e:
                print(f"   ‚ùå Error cargando {eval_file.name}: {e}")
        
        print(f"\nüìä Resultados cargados:")
        print(f"   üìà Estudios: {len(self.studies)}")
        print(f"   üìã Res√∫menes: {len(self.summaries)}")
        print(f"   üß™ Evaluaciones: {len(self.evaluations)}")
    
    def create_comparison_dataframe(self) -> pd.DataFrame:
        """
        Crear DataFrame con comparaci√≥n de todos los resultados
        """
        comparison_data = []
        
        for summary in self.summaries:
            timestamp = summary['timestamp']
            
            for model_name, score in summary['best_scores'].items():
                params = summary['best_params'][model_name]
                
                # Encontrar evaluaci√≥n correspondiente
                eval_data = None
                for evaluation in self.evaluations:
                    if model_name in evaluation:
                        eval_data = evaluation[model_name]
                        break
                
                row = {
                    'timestamp': timestamp,
                    'model': model_name,
                    'cv_score': score,
                    'val_auc': eval_data.get('val_auc') if eval_data else None,
                    'test_auc': eval_data.get('test_auc') if eval_data else None,
                    'n_estimators': params.get('n_estimators'),
                    'max_depth': params.get('max_depth', params.get('depth')),
                    'learning_rate': params.get('learning_rate'),
                }
                
                comparison_data.append(row)
        
        return pd.DataFrame(comparison_data)
    
    def generate_performance_report(self):
        """
        Generar reporte detallado de performance
        """
        print("\nüìä======================================================================")
        print("üìä REPORTE DE PERFORMANCE DETALLADO")
        print("üìä======================================================================")
        
        if not self.summaries:
            print("‚ùå No hay datos para generar reporte")
            return
        
        df = self.create_comparison_dataframe()
        
        if df.empty:
            print("‚ùå No se pudo crear DataFrame de comparaci√≥n")
            return
        
        # Estad√≠sticas generales
        print("\nüìà ESTAD√çSTICAS GENERALES:")
        print(f"   üî¢ Total de experimentos: {len(df)}")
        print(f"   ü§ñ Modelos √∫nicos: {df['model'].nunique()}")
        print(f"   üìÖ Per√≠odo de experimentos: {df['timestamp'].min()} - {df['timestamp'].max()}")
        
        # Mejores resultados por modelo
        print("\nüèÜ MEJORES RESULTADOS POR MODELO:")
        best_by_model = df.loc[df.groupby('model')['cv_score'].idxmax()]
        
        for _, row in best_by_model.iterrows():
            print(f"\n   üîπ {row['model'].upper()}:")
            print(f"      üìä CV Score: {row['cv_score']:.4f}")
            if pd.notna(row['val_auc']):
                print(f"      üìä Validation AUC: {row['val_auc']:.4f}")
            if pd.notna(row['test_auc']):
                print(f"      üìä Test AUC: {row['test_auc']:.4f}")
            print(f"      üìÖ Fecha: {row['timestamp']}")
            
            # Par√°metros principales
            if pd.notna(row['n_estimators']):
                print(f"      üîß n_estimators: {row['n_estimators']}")
            if pd.notna(row['max_depth']):
                print(f"      üîß max_depth: {row['max_depth']}")
            if pd.notna(row['learning_rate']):
                print(f"      üîß learning_rate: {row['learning_rate']:.4f}")
        
        # Modelo global mejor
        best_overall = df.loc[df['cv_score'].idxmax()]
        print(f"\nü•á MEJOR MODELO GLOBAL:")
        print(f"   ü§ñ Modelo: {best_overall['model'].upper()}")
        print(f"   üìä CV Score: {best_overall['cv_score']:.4f}")
        print(f"   üìÖ Fecha: {best_overall['timestamp']}")
        
        # Evoluci√≥n temporal
        print(f"\nüìà EVOLUCI√ìN TEMPORAL:")
        df_sorted = df.sort_values('timestamp')
        for model in df['model'].unique():
            model_data = df_sorted[df_sorted['model'] == model]
            if len(model_data) > 1:
                improvement = model_data['cv_score'].iloc[-1] - model_data['cv_score'].iloc[0]
                print(f"   üìä {model}: {improvement:+.4f} (primer vs √∫ltimo experimento)")
        
        return df
    
    def analyze_hyperparameter_importance(self):
        """
        Analizar importancia de hiperpar√°metros
        """
        print("\nüîß======================================================================")
        print("üîß AN√ÅLISIS DE IMPORTANCIA DE HIPERPAR√ÅMETROS")
        print("üîß======================================================================")
        
        for timestamp, studies in self.studies.items():
            print(f"\nüìÖ Estudios del {timestamp}:")
            
            for model_name, study in studies.items():
                if hasattr(study, 'trials') and len(study.trials) > 0:
                    print(f"\n   ü§ñ {model_name.upper()}:")
                    print(f"      üî¢ Total trials: {len(study.trials)}")
                    print(f"      üèÜ Mejor valor: {study.best_value:.4f}")
                    
                    # Analizar correlaciones de par√°metros con performance
                    trial_data = []
                    for trial in study.trials:
                        if trial.state == optuna.trial.TrialState.COMPLETE:
                            row = {'value': trial.value}
                            row.update(trial.params)
                            trial_data.append(row)
                    
                    if trial_data:
                        trial_df = pd.DataFrame(trial_data)
                        
                        # Calcular correlaciones
                        numeric_cols = trial_df.select_dtypes(include=[float, int]).columns
                        if 'value' in numeric_cols and len(numeric_cols) > 1:
                            correlations = trial_df[numeric_cols].corr()['value'].abs().sort_values(ascending=False)
                            
                            print(f"      üìà Par√°metros m√°s influyentes:")
                            for param, corr in correlations.items():
                                if param != 'value' and corr > 0.1:
                                    print(f"         {param}: {corr:.3f}")
    
    def generate_interactive_visualizations(self):
        """
        Generar visualizaciones interactivas
        """
        if not PLOTTING_AVAILABLE:
            print("‚ö†Ô∏è  Visualizaciones no disponibles (falta plotly)")
            return
        
        print("\nüìà======================================================================")
        print("üìà GENERANDO VISUALIZACIONES INTERACTIVAS")
        print("üìà======================================================================")
        
        viz_dir = self.results_path / "analysis_visualizations"
        viz_dir.mkdir(exist_ok=True)
        
        # 1. Comparaci√≥n de modelos
        df = self.create_comparison_dataframe()
        if not df.empty:
            self._create_model_comparison_plot(df, viz_dir)
            self._create_parameter_analysis_plots(df, viz_dir)
            self._create_evolution_plot(df, viz_dir)
        
        # 2. An√°lisis de estudios individuales
        for timestamp, studies in self.studies.items():
            study_dir = viz_dir / f"studies_{timestamp}"
            study_dir.mkdir(exist_ok=True)
            
            for model_name, study in studies.items():
                self._create_study_visualizations(study, model_name, study_dir)
        
        print(f"   ‚úÖ Visualizaciones guardadas en: {viz_dir}")
    
    def _create_model_comparison_plot(self, df: pd.DataFrame, output_dir: Path):
        """Crear gr√°fico de comparaci√≥n de modelos"""
        try:
            fig = go.Figure()
            
            for model in df['model'].unique():
                model_data = df[df['model'] == model]
                
                fig.add_trace(go.Scatter(
                    x=model_data['timestamp'],
                    y=model_data['cv_score'],
                    mode='markers+lines',
                    name=model.upper(),
                    marker=dict(size=10),
                    hovertemplate="<b>%{fullData.name}</b><br>" +
                                "Fecha: %{x}<br>" +
                                "CV Score: %{y:.4f}<extra></extra>"
                ))
            
            fig.update_layout(
                title="Evoluci√≥n de Performance por Modelo",
                xaxis_title="Timestamp",
                yaxis_title="CV Score (AUC)",
                hovermode='closest',
                height=500
            )
            
            fig.write_html(output_dir / "model_comparison.html")
            print(f"   ‚úÖ Comparaci√≥n de modelos: model_comparison.html")
            
        except Exception as e:
            print(f"   ‚ùå Error creando comparaci√≥n de modelos: {e}")
    
    def _create_parameter_analysis_plots(self, df: pd.DataFrame, output_dir: Path):
        """Crear gr√°ficos de an√°lisis de par√°metros"""
        try:
            # Gr√°fico de learning rate vs performance
            fig_lr = px.scatter(
                df, 
                x='learning_rate', 
                y='cv_score',
                color='model',
                title="Learning Rate vs Performance",
                hover_data=['max_depth', 'n_estimators']
            )
            fig_lr.write_html(output_dir / "learning_rate_analysis.html")
            
            # Gr√°fico de max_depth vs performance
            fig_depth = px.scatter(
                df,
                x='max_depth',
                y='cv_score', 
                color='model',
                title="Max Depth vs Performance",
                hover_data=['learning_rate', 'n_estimators']
            )
            fig_depth.write_html(output_dir / "max_depth_analysis.html")
            
            print(f"   ‚úÖ An√°lisis de par√°metros: learning_rate_analysis.html, max_depth_analysis.html")
            
        except Exception as e:
            print(f"   ‚ùå Error creando an√°lisis de par√°metros: {e}")
    
    def _create_evolution_plot(self, df: pd.DataFrame, output_dir: Path):
        """Crear gr√°fico de evoluci√≥n temporal"""
        try:
            fig = px.line(
                df,
                x='timestamp',
                y='cv_score',
                color='model',
                title="Evoluci√≥n Temporal de Performance",
                markers=True
            )
            
            fig.update_layout(
                height=500,
                xaxis_title="Timestamp",
                yaxis_title="CV Score (AUC)"
            )
            
            fig.write_html(output_dir / "temporal_evolution.html")
            print(f"   ‚úÖ Evoluci√≥n temporal: temporal_evolution.html")
            
        except Exception as e:
            print(f"   ‚ùå Error creando evoluci√≥n temporal: {e}")
    
    def _create_study_visualizations(self, study, model_name: str, output_dir: Path):
        """Crear visualizaciones para un estudio espec√≠fico"""
        try:
            if not hasattr(study, 'trials') or len(study.trials) == 0:
                return
            
            # Historia de optimizaci√≥n
            from optuna.visualization import plot_optimization_history
            fig_history = plot_optimization_history(study)
            fig_history.write_html(output_dir / f"{model_name}_optimization_history.html")
            
            # Importancia de par√°metros
            from optuna.visualization import plot_param_importances
            fig_importance = plot_param_importances(study)
            fig_importance.write_html(output_dir / f"{model_name}_param_importances.html")
            
            print(f"   ‚úÖ Visualizaciones de {model_name}: {output_dir.name}")
            
        except Exception as e:
            print(f"   ‚ùå Error creando visualizaciones de {model_name}: {e}")
    
    def export_best_configs(self, output_file: Optional[str] = None):
        """
        Exportar las mejores configuraciones encontradas
        """
        print("\nüíæ======================================================================")
        print("üíæ EXPORTANDO MEJORES CONFIGURACIONES")
        print("üíæ======================================================================")
        
        if output_file is None:
            output_file = self.results_path / f"best_configs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        best_configs = {}
        
        # Encontrar la mejor configuraci√≥n para cada modelo
        df = self.create_comparison_dataframe()
        if not df.empty:
            best_by_model = df.loc[df.groupby('model')['cv_score'].idxmax()]
            
            for _, row in best_by_model.iterrows():
                model = row['model']
                timestamp = row['timestamp']
                
                # Buscar los par√°metros completos
                for summary in self.summaries:
                    if summary['timestamp'] == timestamp and model in summary['best_params']:
                        best_configs[model] = {
                            'cv_score': row['cv_score'],
                            'val_auc': row['val_auc'] if pd.notna(row['val_auc']) else None,
                            'test_auc': row['test_auc'] if pd.notna(row['test_auc']) else None,
                            'timestamp': timestamp,
                            'parameters': summary['best_params'][model]
                        }
                        break
        
        # Guardar configuraciones
        with open(output_file, 'w') as f:
            json.dump(best_configs, f, indent=2, default=str)
        
        print(f"‚úÖ Mejores configuraciones exportadas a: {output_file}")
        
        # Mostrar resumen
        for model, config in best_configs.items():
            print(f"\nüîπ {model.upper()}:")
            print(f"   üìä CV Score: {config['cv_score']:.4f}")
            if config['val_auc']:
                print(f"   üìä Validation AUC: {config['val_auc']:.4f}")
            if config['test_auc']:
                print(f"   üìä Test AUC: {config['test_auc']:.4f}")
            print(f"   üìÖ Fecha: {config['timestamp']}")
        
        return best_configs
    
    def generate_full_report(self):
        """
        Generar reporte completo de an√°lisis
        """
        print("üöÄ======================================================================")
        print("üöÄ GENERANDO REPORTE COMPLETO DE AN√ÅLISIS")
        print("üöÄ======================================================================")
        
        # 1. Reporte de performance
        df = self.generate_performance_report()
        
        # 2. An√°lisis de hiperpar√°metros
        self.analyze_hyperparameter_importance()
        
        # 3. Visualizaciones
        self.generate_interactive_visualizations()
        
        # 4. Exportar mejores configs
        best_configs = self.export_best_configs()
        
        # 5. Resumen final
        print("\n‚úÖ======================================================================")
        print("‚úÖ AN√ÅLISIS COMPLETADO")
        print("‚úÖ======================================================================")
        print(f"üìÅ Todos los resultados en: {self.results_path}")
        print(f"üìà Visualizaciones en: {self.results_path}/analysis_visualizations")
        
        return {
            'performance_df': df,
            'best_configs': best_configs,
            'studies_count': len(self.studies),
            'summaries_count': len(self.summaries)
        }

def main():
    """
    Funci√≥n principal para an√°lisis completo
    """
    print("üîç ANALIZADOR DE RESULTADOS OPTUNA")
    print("üîç OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS")
    print("üîç======================================================================")
    
    analyzer = OptunaResultsAnalyzer()
    results = analyzer.generate_full_report()
    
    print(f"\nüéØ An√°lisis completado exitosamente!")
    return results

if __name__ == "__main__":
    main()
