#!/usr/bin/env python3
"""
Script de demostraciÃ³n para funcionalidades de Fase 3
Muestra paralelizaciÃ³n, gestiÃ³n de memoria y optimizaciÃ³n avanzada
"""

import sys
import os
import time
import json
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Suprimir warnings
warnings.filterwarnings('ignore')

# Agregar path para imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def demo_parallelization():
    """DemostraciÃ³n del sistema de paralelizaciÃ³n"""
    print("ðŸš€======================================================================")
    print("ðŸ‘¥ DEMOSTRACIÃ“N DE PARALELIZACIÃ“N")
    print("ðŸš€======================================================================")
    
    try:
        from utils.parallelization import (
            WorkerManager, DistributedOptimizer, ParallelTrialExecutor,
            ParallelizationConfig, DEFAULT_PARALLELIZATION_CONFIG
        )
        
        print("ðŸ“‹ ConfiguraciÃ³n de paralelizaciÃ³n:")
        config = DEFAULT_PARALLELIZATION_CONFIG
        print(f"   ðŸ‘¥ Workers: {config.n_workers}")
        print(f"   ðŸ”„ Tipo: {config.worker_type}")
        print(f"   ðŸ“¦ TamaÃ±o de cola: {config.queue_size}")
        print(f"   â° Timeout: {config.timeout}s")
        
        # DemostraciÃ³n de WorkerManager
        print("\nðŸ”§ Iniciando WorkerManager...")
        worker_manager = WorkerManager(config)
        worker_manager.start_workers()
        
        # FunciÃ³n de ejemplo computacionalmente intensiva
        def complex_computation(n):
            """SimulaciÃ³n de cÃ³mputo complejo"""
            result = 0
            for i in range(n * 1000):
                result += i ** 0.5
            return result
        
        # Test secuencial vs paralelo
        print("\nâ±ï¸ ComparaciÃ³n: Secuencial vs Paralelo")
        
        # EjecuciÃ³n secuencial
        start_time = time.time()
        sequential_results = []
        for i in range(1, 6):
            result = complex_computation(i)
            sequential_results.append(result)
        sequential_time = time.time() - start_time
        
        print(f"   ðŸ“Š Secuencial: {sequential_time:.2f}s")
        
        # EjecuciÃ³n paralela
        start_time = time.time()
        tasks = [(complex_computation, (i,), {}) for i in range(1, 6)]
        parallel_results = worker_manager.submit_batch(tasks)
        parallel_time = time.time() - start_time
        
        print(f"   ðŸš€ Paralelo: {parallel_time:.2f}s")
        
        if sequential_time > 0:
            speedup = sequential_time / parallel_time
            print(f"   âš¡ AceleraciÃ³n: {speedup:.1f}x")
        
        # EstadÃ­sticas de workers
        stats = worker_manager.get_stats()
        print(f"\nðŸ“ˆ EstadÃ­sticas de Workers:")
        print(f"   ðŸ‘¥ Workers activos: {stats['n_workers']}")
        print(f"   âœ… Trials completados: {stats['total_trials']}")
        print(f"   âŒ Failures: {stats['total_failures']}")
        
        worker_manager.stop_workers()
        print("   âœ… Workers detenidos")
        
        # DemostraciÃ³n de optimizaciÃ³n distribuida
        print("\nðŸŒ Sistema de OptimizaciÃ³n Distribuida:")
        distributed_optimizer = DistributedOptimizer(config)
        distributed_optimizer.start()
        
        # Simular distribuciÃ³n de trials
        mock_trials = [f"trial_{i}" for i in range(10)]
        print(f"   ðŸ“¦ Distribuyendo {len(mock_trials)} trials...")
        
        # Simular ejecuciÃ³n distribuida
        start_time = time.time()
        distributed_results = distributed_optimizer.distribute_trials(
            [(lambda x: f"result_{x}", (trial,), {}) for trial in mock_trials]
        )
        distributed_time = time.time() - start_time
        
        print(f"   â±ï¸ Tiempo distribuido: {distributed_time:.2f}s")
        print(f"   ðŸ“Š Resultados: {len(distributed_results)} trials completados")
        
        # EstadÃ­sticas del cluster
        cluster_stats = distributed_optimizer.get_cluster_stats()
        print(f"   ðŸ–¥ï¸ Nodos totales: {cluster_stats['total_nodes']}")
        print(f"   ðŸŒ Modo distribuido: {cluster_stats['distributed_mode']}")
        
        distributed_optimizer.stop()
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en demostraciÃ³n de paralelizaciÃ³n: {e}")
        return False

def demo_memory_management():
    """DemostraciÃ³n del sistema de gestiÃ³n de memoria"""
    print("\nðŸš€======================================================================")
    print("ðŸ§  DEMOSTRACIÃ“N DE GESTIÃ“N DE MEMORIA")
    print("ðŸš€======================================================================")
    
    try:
        from utils.memory_manager import (
            MemoryManager, MemoryMonitor, CacheManager, 
            MemoryConfig, DEFAULT_MEMORY_CONFIG
        )
        
        print("ðŸ“‹ ConfiguraciÃ³n de memoria:")
        config = DEFAULT_MEMORY_CONFIG
        print(f"   ðŸ’¾ LÃ­mite de memoria: {config.memory_limit_mb}MB")
        print(f"   ðŸ§¹ Threshold GC: {config.gc_threshold_mb}MB")
        print(f"   ðŸ“¦ TamaÃ±o de chunk: {config.chunk_size_mb}MB")
        print(f"   ðŸ—„ï¸ Cache habilitado: {config.cache_enabled}")
        
        # Crear memory manager
        memory_manager = MemoryManager(config)
        memory_manager.start()
        
        print("\nðŸ“Š Estado inicial de memoria:")
        initial_stats = memory_manager.get_comprehensive_stats()
        memory_stats = initial_stats['memory_stats']
        print(f"   ðŸ“ˆ Uso actual: {memory_stats['used_percent']:.1f}%")
        print(f"   ðŸ’¾ Memoria usada: {memory_stats['used_mb']:.0f}MB")
        print(f"   ðŸ”„ Objetos GC: {memory_stats['gc_objects']}")
        
        # DemostraciÃ³n de cache
        print("\nðŸ—„ï¸ DemostraciÃ³n de Cache:")
        cache_manager = memory_manager.cache_manager
        
        # Guardar algunos valores en cache
        test_data = {
            "model_xgb": {"n_estimators": 100, "max_depth": 6, "score": 0.85},
            "model_lgb": {"n_estimators": 150, "max_depth": 4, "score": 0.83},
            "model_cat": {"n_estimators": 200, "depth": 5, "score": 0.87}
        }
        
        for key, data in test_data.items():
            cache_manager.set(key, data, metadata={"timestamp": time.time()})
        
        print(f"   ðŸ’¾ Datos guardados en cache: {len(test_data)} elementos")
        
        # Recuperar datos del cache
        for key in test_data.keys():
            cached_data = cache_manager.get(key)
            if cached_data:
                print(f"   âœ… Cache hit para {key}: score={cached_data['score']}")
            else:
                print(f"   âŒ Cache miss para {key}")
        
        # EstadÃ­sticas de cache
        cache_stats = cache_manager.get_stats()
        print(f"   ðŸ“Š Cache hits: {cache_stats['hits']}")
        print(f"   ðŸ“Š Cache misses: {cache_stats['misses']}")
        print(f"   ðŸ“Š Hit rate: {cache_stats['hit_rate']:.1%}")
        
        # DemostraciÃ³n de procesamiento por chunks
        print("\nðŸ“¦ DemostraciÃ³n de Procesamiento por Chunks:")
        
        # Crear dataset grande simulado
        large_dataset = list(range(10000))
        print(f"   ðŸ“Š Dataset: {len(large_dataset)} elementos")
        
        def process_chunk(chunk):
            """Procesar chunk con operaciÃ³n intensiva"""
            return {
                'size': len(chunk),
                'sum': sum(chunk),
                'mean': sum(chunk) / len(chunk),
                'max': max(chunk),
                'min': min(chunk)
            }
        
        # Procesar por chunks
        start_time = time.time()
        chunk_results = memory_manager.chunk_processor.process_data_chunks(
            large_dataset, process_chunk, chunk_size=1000
        )
        chunk_time = time.time() - start_time
        
        print(f"   â±ï¸ Tiempo de procesamiento: {chunk_time:.2f}s")
        print(f"   ðŸ“¦ Chunks procesados: {len(chunk_results)}")
        
        # Resumen de chunks
        total_elements = sum(r['size'] for r in chunk_results if r)
        avg_chunk_size = total_elements / len(chunk_results) if chunk_results else 0
        print(f"   ðŸ“Š Elementos totales: {total_elements}")
        print(f"   ðŸ“Š TamaÃ±o promedio de chunk: {avg_chunk_size:.0f}")
        
        # DemostraciÃ³n de garbage collection
        print("\nðŸ§¹ DemostraciÃ³n de Garbage Collection:")
        
        # Crear objetos temporales para forzar GC
        temp_objects = []
        for i in range(1000):
            temp_objects.append([j for j in range(100)])
        
        print("   ðŸ“Š Objetos temporales creados")
        
        # Ejecutar GC estratÃ©gico
        gc_result = memory_manager.gc_manager.strategic_gc(force=True)
        print(f"   ðŸ§¹ GC ejecutado: {gc_result['memory_freed']:.1f}MB liberados")
        print(f"   â±ï¸ Tiempo de GC: {gc_result['duration']:.3f}s")
        print(f"   ðŸ“Š Objetos recolectados: {gc_result['total_collected']}")
        
        # Limpiar referencias
        del temp_objects
        
        # Estado final de memoria
        print("\nðŸ“Š Estado final de memoria:")
        final_stats = memory_manager.get_comprehensive_stats()
        final_memory_stats = final_stats['memory_stats']
        print(f"   ðŸ“ˆ Uso final: {final_memory_stats['used_percent']:.1f}%")
        print(f"   ðŸ’¾ Memoria usada: {final_memory_stats['used_mb']:.0f}MB")
        
        # Diferencia de memoria
        memory_diff = memory_stats['used_mb'] - final_memory_stats['used_mb']
        print(f"   ðŸ“‰ Memoria liberada: {memory_diff:.1f}MB")
        
        memory_manager.stop()
        print("   âœ… GestiÃ³n de memoria detenida")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en demostraciÃ³n de gestiÃ³n de memoria: {e}")
        return False

def demo_cache_performance():
    """DemostraciÃ³n de rendimiento del cache"""
    print("\nðŸš€======================================================================")
    print("ðŸ—„ï¸ DEMOSTRACIÃ“N DE RENDIMIENTO DE CACHE")
    print("ðŸš€======================================================================")
    
    try:
        from utils.memory_manager import CacheManager, MemoryConfig
        
        config = MemoryConfig(
            cache_enabled=True,
            cache_ttl_hours=24,
            cache_dir="demo_cache"
        )
        
        cache_manager = CacheManager(config)
        
        # SimulaciÃ³n de resultados de optimizaciÃ³n costosos
        def expensive_computation(model_name, params):
            """Simular cÃ³mputo costoso de optimizaciÃ³n"""
            time.sleep(0.1)  # Simular tiempo de cÃ³mputo
            score = sum(params.values()) / len(params) * 0.001  # Score simulado
            return {
                'model': model_name,
                'params': params,
                'score': score,
                'computation_time': 0.1
            }
        
        # Configuraciones de modelos para test
        model_configs = [
            ('xgboost', {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1}),
            ('lightgbm', {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.05}),
            ('catboost', {'n_estimators': 200, 'depth': 5, 'learning_rate': 0.03}),
        ]
        
        print("â±ï¸ ComparaciÃ³n: Sin Cache vs Con Cache")
        
        # Primera ejecuciÃ³n (sin cache)
        print("\nðŸ”„ Primera ejecuciÃ³n (sin cache):")
        start_time = time.time()
        
        results_no_cache = []
        for model_name, params in model_configs:
            result = expensive_computation(model_name, params)
            results_no_cache.append(result)
            
            # Guardar en cache para segunda ejecuciÃ³n
            cache_key = cache_manager.generate_key(model_name, **params)
            cache_manager.set(cache_key, result)
            
        no_cache_time = time.time() - start_time
        print(f"   â±ï¸ Tiempo sin cache: {no_cache_time:.2f}s")
        
        # Segunda ejecuciÃ³n (con cache)
        print("\nðŸ—„ï¸ Segunda ejecuciÃ³n (con cache):")
        start_time = time.time()
        
        results_with_cache = []
        cache_hits = 0
        
        for model_name, params in model_configs:
            cache_key = cache_manager.generate_key(model_name, **params)
            cached_result = cache_manager.get(cache_key)
            
            if cached_result:
                results_with_cache.append(cached_result)
                cache_hits += 1
                print(f"   âœ… Cache hit para {model_name}")
            else:
                result = expensive_computation(model_name, params)
                results_with_cache.append(result)
                print(f"   âŒ Cache miss para {model_name}")
        
        cache_time = time.time() - start_time
        print(f"   â±ï¸ Tiempo con cache: {cache_time:.2f}s")
        
        # AnÃ¡lisis de rendimiento
        if no_cache_time > 0:
            speedup = no_cache_time / cache_time
            time_saved = no_cache_time - cache_time
            
            print(f"\nðŸ“Š AnÃ¡lisis de Rendimiento:")
            print(f"   ðŸš€ AceleraciÃ³n: {speedup:.1f}x")
            print(f"   â° Tiempo ahorrado: {time_saved:.2f}s")
            print(f"   ðŸŽ¯ Cache hits: {cache_hits}/{len(model_configs)}")
            print(f"   ðŸ“ˆ Hit rate: {cache_hits/len(model_configs)*100:.1f}%")
        
        # EstadÃ­sticas finales del cache
        cache_stats = cache_manager.get_stats()
        print(f"\nðŸ“Š EstadÃ­sticas del Cache:")
        print(f"   ðŸŽ¯ Total hits: {cache_stats['hits']}")
        print(f"   âŒ Total misses: {cache_stats['misses']}")
        print(f"   ðŸ“ˆ Hit rate global: {cache_stats['hit_rate']:.1%}")
        print(f"   ðŸ’¾ Elementos en memoria: {cache_stats['memory_cache_size']}")
        
        # Limpiar cache
        cache_manager.clear()
        print("   ðŸ§¹ Cache limpiado")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en demostraciÃ³n de cache: {e}")
        return False

def demo_integrated_optimization():
    """DemostraciÃ³n de optimizaciÃ³n integrada con Fase 3"""
    print("\nðŸš€======================================================================")
    print("âš¡ DEMOSTRACIÃ“N DE OPTIMIZACIÃ“N INTEGRADA - FASE 3")
    print("ðŸš€======================================================================")
    
    try:
        from crypto_hyperparameter_optimizer import CryptoHyperparameterOptimizer
        from utils.parallelization import ParallelizationConfig
        from utils.memory_manager import MemoryConfig
        
        # Configuraciones optimizadas para demo
        parallel_config = ParallelizationConfig(
            n_workers=2,  # Reducido para demo
            worker_type='process',
            queue_size=100,
            timeout=30,
            memory_limit_mb=1024,
            max_retries=2
        )
        
        memory_config = MemoryConfig(
            memory_limit_mb=2048,
            gc_threshold_mb=1536,
            chunk_size_mb=256,
            cache_enabled=True,
            cache_ttl_hours=1,
            monitor_interval=5,
            log_memory_usage=True
        )
        
        print("ðŸ“‹ ConfiguraciÃ³n de optimizaciÃ³n integrada:")
        print(f"   ðŸ‘¥ Workers: {parallel_config.n_workers}")
        print(f"   ðŸ’¾ LÃ­mite memoria: {memory_config.memory_limit_mb}MB")
        print(f"   ðŸ—„ï¸ Cache habilitado: {memory_config.cache_enabled}")
        
        # Crear optimizador con configuraciones de Fase 3
        print("\nðŸ”§ Creando optimizador con Fase 3...")
        optimizer = CryptoHyperparameterOptimizer(
            parallelization_config=parallel_config,
            memory_config=memory_config
        )
        
        # Verificar disponibilidad de componentes
        stats = optimizer.get_system_stats()
        print(f"   ðŸŽ¯ Fase 1: {'âœ…' if stats['phase_1_enabled'] else 'âŒ'}")
        print(f"   ðŸš€ Fase 2: {'âœ…' if stats['phase_2_enabled'] else 'âŒ'}")
        print(f"   âš¡ Fase 3: {'âœ…' if stats['phase_3_enabled'] else 'âŒ'}")
        
        if stats['phase_3_enabled']:
            print("\nðŸ“Š EstadÃ­sticas del sistema:")
            
            # EstadÃ­sticas de memoria
            if 'memory_stats' in stats:
                memory_info = stats['memory_stats']['memory_stats']
                print(f"   ðŸ§  Memoria: {memory_info['used_percent']:.1f}% "
                      f"({memory_info['used_mb']:.0f}MB)")
            
            # EstadÃ­sticas de workers
            if 'worker_stats' in stats:
                worker_info = stats['worker_stats']
                print(f"   ðŸ‘¥ Workers: {worker_info['n_workers']} disponibles")
        
        # Simular optimizaciÃ³n con gestiÃ³n avanzada
        print("\nðŸŽ¯ Simulando optimizaciÃ³n con gestiÃ³n avanzada...")
        
        def simulate_optimization_task(model_name, trial_id):
            """Simular tarea de optimizaciÃ³n"""
            # Simular trabajo computacional
            time.sleep(0.1)
            
            # Simular resultado
            score = 0.7 + (trial_id % 10) * 0.02  # Score entre 0.7 y 0.88
            
            return {
                'model': model_name,
                'trial_id': trial_id,
                'score': score,
                'params': {
                    'n_estimators': 100 + trial_id * 10,
                    'max_depth': 3 + trial_id % 5
                }
            }
        
        # Ejecutar optimizaciÃ³n simulada con workers
        if optimizer.worker_manager:
            optimizer.worker_manager.start_workers()
            
            # Crear tareas de optimizaciÃ³n
            optimization_tasks = []
            for model in ['XGBoost', 'LightGBM', 'CatBoost']:
                for trial in range(3):  # 3 trials por modelo para demo
                    optimization_tasks.append(
                        (simulate_optimization_task, (model, trial), {})
                    )
            
            print(f"   ðŸ“¦ Ejecutando {len(optimization_tasks)} tareas de optimizaciÃ³n...")
            
            # Ejecutar en paralelo
            start_time = time.time()
            results = optimizer.worker_manager.submit_batch(optimization_tasks)
            execution_time = time.time() - start_time
            
            print(f"   â±ï¸ Tiempo de ejecuciÃ³n: {execution_time:.2f}s")
            print(f"   âœ… Tareas completadas: {len([r for r in results if r])}")
            
            # Agrupar resultados por modelo
            model_results = {}
            for result in results:
                if result:
                    model = result['model']
                    if model not in model_results:
                        model_results[model] = []
                    model_results[model].append(result)
            
            # Mostrar mejores resultados por modelo
            print(f"\nðŸ† Mejores resultados por modelo:")
            for model, results_list in model_results.items():
                best_result = max(results_list, key=lambda x: x['score'])
                print(f"   {model}: {best_result['score']:.3f} "
                      f"(trial {best_result['trial_id']})")
            
            optimizer.worker_manager.stop_workers()
        
        # Demostrar gestiÃ³n de memoria durante optimizaciÃ³n
        if optimizer.memory_manager:
            print(f"\nðŸ§  GestiÃ³n de memoria durante optimizaciÃ³n:")
            
            # Estado inicial
            initial_stats = optimizer.memory_manager.get_comprehensive_stats()
            initial_memory = initial_stats['memory_stats']
            print(f"   ðŸ“Š Memoria inicial: {initial_memory['used_percent']:.1f}%")
            
            # Simular carga de trabajo pesada
            heavy_data = []
            for i in range(1000):
                heavy_data.append([j for j in range(100)])
            
            print("   ðŸ“¦ Carga de trabajo pesada creada")
            
            # Ejecutar optimizaciÃ³n de memoria
            opt_result = optimizer.memory_manager.optimize_memory()
            print(f"   ðŸ§¹ OptimizaciÃ³n ejecutada: "
                  f"{opt_result['gc_result']['memory_freed']:.1f}MB liberados")
            
            # Estado final
            final_stats = optimizer.memory_manager.get_comprehensive_stats()
            final_memory = final_stats['memory_stats']
            print(f"   ðŸ“Š Memoria final: {final_memory['used_percent']:.1f}%")
            
            # Limpiar
            del heavy_data
        
        # Limpiar recursos
        optimizer.cleanup_resources()
        print("   âœ… Recursos limpiados")
        
        print(f"\nðŸŽ‰ DemostraciÃ³n de optimizaciÃ³n integrada completada!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en demostraciÃ³n integrada: {e}")
        return False

def demo_performance_comparison():
    """DemostraciÃ³n de comparaciÃ³n de rendimiento"""
    print("\nðŸš€======================================================================")
    print("ðŸ“Š COMPARACIÃ“N DE RENDIMIENTO - FASE 3 vs BÃSICO")
    print("ðŸš€======================================================================")
    
    try:
        from utils.parallelization import WorkerManager, ParallelizationConfig
        from utils.memory_manager import MemoryManager, MemoryConfig
        
        # FunciÃ³n de trabajo intensivo
        def intensive_computation(n):
            """FunciÃ³n computacionalmente intensiva"""
            result = 0
            for i in range(n * 10000):
                result += (i ** 0.5) % 1000
            return result
        
        # Datos de test
        test_sizes = [1, 2, 3, 4, 5]
        
        print("â±ï¸ Ejecutando comparaciÃ³n de rendimiento...")
        
        # 1. EjecuciÃ³n secuencial bÃ¡sica
        print("\nðŸŒ EjecuciÃ³n secuencial bÃ¡sica:")
        start_time = time.time()
        sequential_results = []
        for size in test_sizes:
            result = intensive_computation(size)
            sequential_results.append(result)
        sequential_time = time.time() - start_time
        print(f"   â±ï¸ Tiempo: {sequential_time:.2f}s")
        
        # 2. EjecuciÃ³n paralela con Fase 3
        print("\nðŸš€ EjecuciÃ³n paralela (Fase 3):")
        
        # Configurar paralelizaciÃ³n
        parallel_config = ParallelizationConfig(
            n_workers=3,
            worker_type='process',
            timeout=30
        )
        
        worker_manager = WorkerManager(parallel_config)
        worker_manager.start_workers()
        
        start_time = time.time()
        tasks = [(intensive_computation, (size,), {}) for size in test_sizes]
        parallel_results = worker_manager.submit_batch(tasks)
        parallel_time = time.time() - start_time
        
        print(f"   â±ï¸ Tiempo: {parallel_time:.2f}s")
        
        worker_manager.stop_workers()
        
        # 3. EjecuciÃ³n con gestiÃ³n de memoria
        print("\nðŸ§  EjecuciÃ³n con gestiÃ³n de memoria:")
        
        memory_config = MemoryConfig(
            memory_limit_mb=1024,
            gc_threshold_mb=512,
            cache_enabled=True
        )
        
        memory_manager = MemoryManager(memory_config)
        memory_manager.start()
        
        # Simular uso de cache
        start_time = time.time()
        cached_results = []
        
        for size in test_sizes:
            # Generar key de cache
            cache_key = memory_manager.cache_manager.generate_key("computation", size=size)
            
            # Buscar en cache
            cached_result = memory_manager.cache_manager.get(cache_key)
            if cached_result:
                cached_results.append(cached_result)
            else:
                # Computar y guardar en cache
                result = intensive_computation(size)
                memory_manager.cache_manager.set(cache_key, result)
                cached_results.append(result)
        
        # Segunda pasada (deberÃ­a usar cache)
        for size in test_sizes:
            cache_key = memory_manager.cache_manager.generate_key("computation", size=size)
            cached_result = memory_manager.cache_manager.get(cache_key)
            # Esta vez deberÃ­a estar en cache
        
        memory_time = time.time() - start_time
        print(f"   â±ï¸ Tiempo: {memory_time:.2f}s")
        
        # EstadÃ­sticas de cache
        cache_stats = memory_manager.cache_manager.get_stats()
        print(f"   ðŸŽ¯ Cache hit rate: {cache_stats['hit_rate']:.1%}")
        
        memory_manager.stop()
        
        # AnÃ¡lisis de rendimiento
        print(f"\nðŸ“Š ANÃLISIS DE RENDIMIENTO:")
        print(f"   ðŸŒ Secuencial: {sequential_time:.2f}s (baseline)")
        
        if sequential_time > 0:
            parallel_speedup = sequential_time / parallel_time
            memory_speedup = sequential_time / memory_time
            
            print(f"   ðŸš€ Paralelo: {parallel_time:.2f}s "
                  f"(aceleraciÃ³n: {parallel_speedup:.1f}x)")
            print(f"   ðŸ§  Con cache: {memory_time:.2f}s "
                  f"(aceleraciÃ³n: {memory_speedup:.1f}x)")
            
            # Mejor configuraciÃ³n
            best_time = min(parallel_time, memory_time)
            best_speedup = sequential_time / best_time
            best_method = "Paralelo" if parallel_time < memory_time else "Cache"
            
            print(f"\nðŸ† MEJOR RENDIMIENTO:")
            print(f"   ðŸ¥‡ MÃ©todo: {best_method}")
            print(f"   âš¡ AceleraciÃ³n: {best_speedup:.1f}x")
            print(f"   ðŸ’¾ Tiempo ahorrado: {sequential_time - best_time:.2f}s")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n de rendimiento: {e}")
        return False

def main():
    """Ejecutar todas las demostraciones de Fase 3"""
    print("ðŸš€======================================================================")
    print("ðŸŽª DEMOSTRACIÃ“N COMPLETA DE FASE 3 - EFICIENCIA Y ESCALABILIDAD")
    print("ðŸš€======================================================================")
    print(f"ðŸ• Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    demos = [
        ("ParalelizaciÃ³n", demo_parallelization),
        ("GestiÃ³n de Memoria", demo_memory_management),
        ("Rendimiento de Cache", demo_cache_performance),
        ("OptimizaciÃ³n Integrada", demo_integrated_optimization),
        ("ComparaciÃ³n de Rendimiento", demo_performance_comparison)
    ]
    
    results = {}
    total_start_time = time.time()
    
    for demo_name, demo_func in demos:
        print(f"\n{'='*20} {demo_name.upper()} {'='*20}")
        
        try:
            start_time = time.time()
            success = demo_func()
            execution_time = time.time() - start_time
            
            results[demo_name] = {
                'success': success,
                'execution_time': execution_time
            }
            
            status = "âœ… EXITOSA" if success else "âŒ FALLIDA"
            print(f"\n{status} - {demo_name} completada en {execution_time:.2f}s")
            
        except Exception as e:
            execution_time = time.time() - start_time
            results[demo_name] = {
                'success': False,
                'execution_time': execution_time,
                'error': str(e)
            }
            print(f"\nâŒ ERROR en {demo_name}: {e}")
    
    # Resumen final
    total_time = time.time() - total_start_time
    successful_demos = sum(1 for r in results.values() if r['success'])
    total_demos = len(demos)
    
    print(f"\nðŸš€======================================================================")
    print(f"ðŸ“Š RESUMEN DE DEMOSTRACIONES DE FASE 3")
    print(f"ðŸš€======================================================================")
    print(f"ðŸ• Tiempo total: {total_time:.2f}s")
    print(f"âœ… Demos exitosas: {successful_demos}/{total_demos}")
    print(f"ðŸ“ˆ Tasa de Ã©xito: {successful_demos/total_demos*100:.1f}%")
    
    print(f"\nðŸ“‹ Detalles por demostraciÃ³n:")
    for demo_name, result in results.items():
        status = "âœ…" if result['success'] else "âŒ"
        time_str = f"{result['execution_time']:.2f}s"
        print(f"   {status} {demo_name}: {time_str}")
        
        if not result['success'] and 'error' in result:
            print(f"      Error: {result['error']}")
    
    if successful_demos == total_demos:
        print(f"\nðŸŽ‰ Â¡TODAS LAS DEMOSTRACIONES DE FASE 3 FUERON EXITOSAS!")
        print("ðŸš€ Funcionalidades demostradas:")
        print("   âœ… ParalelizaciÃ³n con multiple workers")
        print("   âœ… OptimizaciÃ³n distribuida")
        print("   âœ… GestiÃ³n inteligente de memoria")
        print("   âœ… Cache de resultados de alta velocidad")
        print("   âœ… Garbage collection estratÃ©gico")
        print("   âœ… Procesamiento por chunks")
        print("   âœ… Persistencia y backup automÃ¡tico")
        print("   âœ… IntegraciÃ³n completa con optimizador")
        print("   âœ… AceleraciÃ³n significativa de rendimiento")
    else:
        print(f"\nâš ï¸ {total_demos - successful_demos} demostraciones fallaron")
        print("ðŸ”§ Revisar configuraciÃ³n y dependencias")
    
    # Crear reporte de demostraciÃ³n
    demo_report = {
        'timestamp': datetime.now().isoformat(),
        'total_execution_time': total_time,
        'successful_demos': successful_demos,
        'total_demos': total_demos,
        'success_rate': successful_demos/total_demos*100,
        'demo_results': results,
        'phase_3_capabilities': [
            'ParalelizaciÃ³n con workers',
            'GestiÃ³n de memoria avanzada',
            'Cache de alta velocidad',
            'Procesamiento por chunks',
            'Garbage collection estratÃ©gico',
            'Persistencia automÃ¡tica',
            'OptimizaciÃ³n distribuida',
            'IntegraciÃ³n completa'
        ]
    }
    
    # Guardar reporte
    report_path = Path("demo_phase3_report.json")
    with open(report_path, 'w') as f:
        json.dump(demo_report, f, indent=2, default=str)
    
    print(f"\nðŸ“„ Reporte completo guardado en: {report_path}")
    print(f"ðŸš€ DemostraciÃ³n de Fase 3 completada!")

if __name__ == "__main__":
    main()
